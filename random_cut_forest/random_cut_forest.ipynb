{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Random Cut Forests\n",
    "\n",
    "***Random Cut Forestを利用した時系列データにおける異常検知*** [[Original (en)](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb)]\n",
    "\n",
    "---\n",
    "\n",
    "1. [はじめに](#Iはじめに)\n",
    "1. [セットアップ](#セットアップ)\n",
    "1. [学習](#学習)\n",
    "1. [推論](#推論)\n",
    "1. [おわりに](#おわりに)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "***\n",
    "\n",
    "\n",
    "Amazon SageMaker Random Cut Forest (RCF) は異常検知のためのアルゴリズムです。異常検知は、例えば、ウェブサイトのトラフィックの予期せぬスパイクや、例年とは異なる温度の変化、想定外の公共交通機関の混雑などを検知するといった利用例があります。\n",
    "\n",
    "このノートブックでは、異常検知のベンチーマークとして用いられるNew York Cityのタクシーデータセットを使い、RCFによって異常スコアを計算し、通常とは異なるイベントを検知します。このノートブックの主な目的は\n",
    "* Amazon SageMaker で使うためのデータを取得・変換・保存する方法を学ぶ、\n",
    "* データセットを用いて RCF モデルを作るために SageMaker トレーニングジョブを作成する、\n",
    "* RCF モデルを使って Amazon SageMaker エンドポイントで推論を行うことです。\n",
    "\n",
    "以下はこのノートブックのゴールには **含まれません**:\n",
    "* RCF モデルを深く理解する、\n",
    "* Amazon SageMaker RCF アルゴリズムがどのように動いているか理解する。\n",
    "\n",
    "RCFに関する詳しいドキュメントは[SageMaker RCF Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html)を参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セットアップ\n",
    "\n",
    "## データのダウンロード\n",
    "\n",
    "New York Cityのタクシーデータセットをダウンロードして`./data`のフォルダにデータを保存します。今回使用するデータは、タクシーの乗客数を30分ごとにカウントした`nyc_taxi.csv`というファイルを利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "    \n",
    "data_dir = 'data/'\n",
    "data_filename = 'nyc_taxi.csv'\n",
    "data_source = 'https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "urllib.request.urlretrieve(data_source, data_dir + data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データをのぞいてみる\n",
    "\n",
    "ダウンロードしたファイルをPandasで開き、matplotlibでグラフ化してみます。グラフ化すると、以下のことがわかると思います。\n",
    "- データに周期性がある。細かい周期は1日です。\n",
    "- 6000ステップのあたりに異常値がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "taxi_data = pd.read_csv(data_dir + data_filename, delimiter=',')\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "taxi_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのアップロード\n",
    "\n",
    "SageMakerをJupyterとして使ってDeep Learningを行うことも可能ですが、**SageMaker Pyhon SDKによって、クラウドの強みを活かした効率のよい学習が可能です。** \n",
    "そのための準備として**学習に必要なデータをS3にアップロード**しましょう。これによって、このnotebookインスタンスよりも高性能なnotebookインスタンスを立ち上げて、S3からデータを取得して高速に学習する、といったことが可能になります。\n",
    "\n",
    "以下では、SageMakerのセッション情報などを取得して、S3にファイルをアップロードします。ファイルを保存するためのS3のバケットは自動で作成され、バケット名がdefault_backet()によって自動設定されるsagemaker-{region}-{AWS account ID}で、prefixは`notebook/rcf/taxi`を指定しています。バケット名も自由に設定できますが、世界中で唯一の名前となるような設定が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Upload files to S3\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'notebook/rcf/taxi'\n",
    "train_input = sess.upload_data(\n",
    "        path=data_dir + data_filename, \n",
    "        key_prefix=prefix)\n",
    "\n",
    "# Show S3 path \n",
    "print(\"Training data is uploaded to\", train_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習\n",
    "\n",
    "## ハイパーパラメータ\n",
    "\n",
    "RCFの主なハイパーパラメータは以下の通りです。\n",
    "\n",
    "* **`num_samples_per_tree`** - RCFは異常検知のための複数の決定木を学習します。各決定木を学習する際、全ての学習データを使うのではなく、ランダムサンプリングを行って、一部の学習データを利用します。ここでは、サンプリングするデータの数を指定します。設定の目安として、異常の発生頻度が`1/num_samples_per_tree`と一致するように設定することが推奨されます。\n",
    "* **`num_trees`** - 構築する決定木の数をここで指定します。適切な値は問題に依存しており、精度の良いモデルを作成するためには、いくつかの値を試す必要があります。決定木の数が多いと学習ならびに推論に時間がかかります。\n",
    "\n",
    "## 学習の実行\n",
    "\n",
    "RCFの学習を実行するため、上記のハイパーパラメータを指定してRandomCutForestを呼び出します。その際、学習データの位置やモデルの出力先、学習用インスタンス等を指定します。学習用インスタンスとして推奨されるのは、`ml.m4`, `ml.c4`, `ml.c5`で、いまのところGPUを考慮した処理は行っていません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RandomCutForest\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=get_execution_role(),\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location=('s3://{}/{}/'+data_dir).format(bucket, prefix),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data.value.as_matrix().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たくさんの情報が出力され、最終的に以下の情報が表示されると学習は完了です。\n",
    "> `===== Job Complete =====`\n",
    "\n",
    "以下のセルを実行すると、先ほど実行した学習ジョブの名前を確認することができます。東京リージョンの場合、以下のURLからジョブの一覧を確認できるので、学習ジョブの名前を選択することで詳細を見ることができます。もし異なるリージョンの場合は、以下のURLを開いた後、該当するリージョンへ変更すると確認できます。\n",
    "\n",
    "https://ap-northeast-1.console.aws.amazon.com/sagemaker/home?region=ap-northeast-1#/jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n",
    "## エンドポイントの作成\n",
    "\n",
    "学習したRCFを異常検知用に使ってみましょう。RCFでは入力データに対して異常スコアを出力するので、異常スコアにもとづいて異常を検知します。異常検知用のエンドポイントは、SageMaker Python SDKの`deploy()`を呼び出すことで作成できます。その際、エンドポイントのインスタンスを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントの作成が終わると、以下のセルからエンドポイントの名前を取得することができます。学習ジョブの一覧と同様にエンドポイントの一覧を下記のページから確認することができます。  \n",
    "    \n",
    "https://ap-northeast-1.console.aws.amazon.com/sagemaker/home?region=ap-northeast-1#/endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(rcf_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 異常を検知するデータの用意\n",
    "\n",
    "エンドポイントができたら、何かデータを用意して異常検知を実行します。エンドポイントが受け取ることができるデータフォーマットは、ビルトインアルゴリズムによって異なり、RCFの場合はCSV, JSON, RecordIO Protobufになります。ここではCSVでデータをエンドポイントに送り、結果をjsonで受け取ることを考えます。そのために、SageMaker Python SDKの`csv_serializer`と`json_deserializer`を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'application/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしたタクシーの乗客数のデータをnumpy arrayの形式に変換してpredictすると結果を得ることができます。numpy arrayを渡すと、上記で設定したserializerによってCSVとしてエンドポイントに渡されます。\n",
    "\n",
    "出力されるjson形式の`score`から異常スコアを取得して、その結果を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_numpy = taxi_data.value.as_matrix().reshape(-1,1)\n",
    "results = rcf_inference.predict(taxi_data_numpy)\n",
    "\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "taxi_data['score'] = pd.Series(scores, index=taxi_data.index)\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "わかりやすいように、以下のセルを実行して、乗客数と異常スコアを同じグラフに表示してみます。6000ステップあたりの大量の乗客数は、やはり異常スコアが高く出ています。一方、乗客が多いからといって、かならずしも異常スコアが高いとは限りません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "#start, end = 5500, 6500\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data_subset['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(taxi_data_subset['score'], color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "異常スコアの降順でソートして、異常スコアのTop-10を表示してみます。この期間には、11/2のNYCマラソンや、1/1のNew Yearが含まれており、いくつかの異常はこのような特定のイベントと関連していることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_subset.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "エンドポイントは実行していると課金がされますので、以下のセルを実行して不要なエンドポイントを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCFの応用ならびに改善（オプション）\n",
    "\n",
    "---\n",
    "\n",
    "## RCFの応用\n",
    "RCFはタクシーの乗客者数における異常検知のように、ある乗客数に対してそれが異常かどうかの判断を行うことができます。一方で、乗客数のトレンドがいつ変化したか、といった異常も検知することができます。\n",
    "\n",
    "\n",
    "## RCFの改善\n",
    "RCFの性能を向上させる方法として、学習モデルを正常とおもわれるデータのみで構成して学習させる方法があります。そうすることで、異常検知の際に、学習した正常のデータから大きく外れるので、精度が改善すると考えられます。\n",
    "\n",
    "またもう一つの手法としてスライディングウィンドウを利用する方法があります。タクシーの乗客者数の場合、RCFの学習では全期間からランダムにデータをサンプリングしていましたが、そうするとサンプリングしたデータの周期性を見失う可能性があります。そこで時系列データを、長さ$P$ステップの連続するデータで切り出すスライディングウィンドウを利用し、連続するデータからランダムでデータをサンプリングする方法をとります。以降のセルでは、その前処理を実行してRCFを学習します。\n",
    "\n",
    "```\n",
    "data = [[x_1],            shingled_data = [[x_1, x_2, ..., x_{P}],\n",
    "        [x_2],    --->                     [x_2, x_3, ..., x_{P+1}],\n",
    "        ...                                ...\n",
    "        [x_N]]                             [x_{N-P}, ..., x_{N}]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shingle(data, shingle_size):\n",
    "    num_data = len(data)\n",
    "    shingled_data = np.zeros((num_data-shingle_size, shingle_size))\n",
    "    \n",
    "    for n in range(num_data - shingle_size):\n",
    "        shingled_data[n] = data[n:(n+shingle_size)]\n",
    "    return shingled_data\n",
    "\n",
    "# single data with shingle size=48 (one day)\n",
    "shingle_size = 48\n",
    "prefix_shingled = 'sagemaker/randomcutforest_shingled'\n",
    "taxi_data_shingled = shingle(taxi_data.values[:,1], shingle_size)\n",
    "print(taxi_data_shingled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=get_execution_role(),\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix_shingled),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix_shingled),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data_shingled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの作成\n",
    "\n",
    "学習したRCFを異常検知用に使うため、エンドポイントを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'appliation/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論の実行\n",
    "\n",
    "先程と同様に推論を実行します。推論に利用するデータが、スライディングウィンドウを利用した`taxi_data_shingled`になることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rcf_inference.predict(taxi_data_shingled)\n",
    "\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "taxi_data['score'] = pd.Series(scores)\n",
    "\n",
    "# Anomaly will be detected in one day because last data for 1 day is used for anomaly detection. \n",
    "taxi_data['timestamp'] = pd.to_datetime(taxi_data['timestamp']) + pd.DateOffset(days=1)\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(scores, color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_subset.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 異常箇所の分析\n",
    "\n",
    "2015年の1月27日の異常スコアが軒並み高くなっています。これはスライディングウィンドウを使わなかった場合は検出できていませんでした。そこで、1月26日から1月30日までの乗客数をグラフ化して見てみます。DataFrameのインデックスの10000から10239が、この5日間のデータに対応しており、異常スコアの高い1月27日は10048から10097までに対応してます。\n",
    "\n",
    "5日間のデータをグラフ化してみると、1月27日を除く4日間は21時頃から乗客数の落ち込みが見られますが、1月27日の乗客数は終日低調です。この状況が異常として検出されていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 10000, 10239\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data_subset['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(taxi_data_subset['score'], color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "エンドポイントは実行していると課金がされますので、以下のセルを実行して不要なエンドポイントを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "nteract": {
   "version": "0.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
