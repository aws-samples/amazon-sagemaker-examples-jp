{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# huggingface/Transformerを用いた事前学習済み日本語BERTモデルによるテキスト分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、[huggingface/Transformer](https://huggingface.co/transformers/)で提供されている[日本語BERT](https://github.com/cl-tohoku/bert-japanese)を用いて日本語のAmazonの商品レビューに対する感情分析を行います。 \n",
    "つまり、そのレビューが Positive (Rating が 5 or 4) か、Negative (Rating が 1 or 2)なのかを判定します。 これは文書を Positive か Negative に分類する2クラスの分類問題として扱うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **データの取得**\n",
    "\n",
    "Amazon の商品レビューデータセットは [Registry of Open Data on AWS](https://registry.opendata.aws/) で公開されており、 \n",
    "以下からダウンロード可能です。\n",
    "このノートブックでは、日本語のデータセットをダウンロードします。\n",
    "- データセットの概要  \n",
    "https://registry.opendata.aws/amazon-reviews/\n",
    "\n",
    "- 日本語のデータセット(readme.htmlからたどることができます）  \n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
    "\n",
    "以下では、データをダウンロードして解凍 (unzip) します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "download_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\" \n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File {} already exists. Skipped download.\".format(file_name))\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    print(\"File downloaded: {}\".format(file_path))\n",
    "    \n",
    "if os.path.exists(tsv_file_path):\n",
    "    print(\"File {} already exists. Skipped unzip.\".format(tsv_file_name))\n",
    "else:\n",
    "    with gzip.open(file_path, mode='rb') as fin:\n",
    "        with open(tsv_file_path, 'wb') as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "            print(\"File uznipped: {}\".format(tsv_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **データの前処理**\n",
    "\n",
    "ダウンロードしたデータには学習に不要なデータや直接利用できないデータもあります。以下の前処理で利用できるようにします。\n",
    "\n",
    "1. ダウンロードしたデータには不要なデータも含まれているので削除し、2クラス分類 (positive が 1, negative が 0)となるように評価データを加工します。\n",
    "2. 学習データ、検証データに分けて、保存します。\n",
    "\n",
    "### **データの加工**\n",
    "\n",
    "今回利用しないデータは以下の４つです。必要なデータだけ選んで保存します。\n",
    "- 評価データ `star_rating` と レビューのテキストデータ `review_body` 以外のデータ\n",
    "- 評価が 3 のデータ (positive でも negative でもないデータ)\n",
    "- 日本語以外のデータ\n",
    "- `review_body`が空白のデータ\n",
    "\n",
    "また、評価が1, 2 のデータはラベル 0 (negative) に、評価が4, 5 のデータはラベル 1 (positive) にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep ='\\t')\n",
    "df_pos_neg = df.loc[:, [\"star_rating\", \"review_body\"]]\n",
    "df_pos_neg = df_pos_neg[df_pos_neg.star_rating != 3]\n",
    "df_pos_neg.loc[df_pos_neg.star_rating < 3, \"star_rating\"] = 0\n",
    "df_pos_neg.loc[df_pos_neg.star_rating > 3, \"star_rating\"] = 1\n",
    "\n",
    "def is_japanese(s):\n",
    "    return True if re.search(r'[ぁ-んァ-ン]', s) else False\n",
    "\n",
    "def is_almost_blank(s):\n",
    "    return True if len(s.strip().split('\\t')) == 2 else False\n",
    "\n",
    "def func_to_row(x):\n",
    "    if not is_japanese(x[\"review_body\"]):\n",
    "        x[\"review_body\"] = np.nan\n",
    "    \n",
    "    elif not is_almost_blank(x[\"review_body\"]):\n",
    "        x[\"review_body\"] = np.nan\n",
    "    \n",
    "    return x\n",
    "\n",
    "labeled_df = df_pos_neg.apply(lambda x: func_to_row(x), axis =1)\n",
    "labeled_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **データの分割**\n",
    "\n",
    "学習には学習データと検証データが必要なため、それぞれ準備します。\n",
    "`train_ratio` で設定した割合のデータを学習データとし、残ったデータを検証データに分割して利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "# Swap positions of \"review_body\",\"star_rating\" because transform.py requires this order.\n",
    "labeled_df = df_pos_neg.loc[:, [\"review_body\",\"star_rating\"]]\n",
    "data_size = len(labeled_df.index)\n",
    "train_ratio = 0.9\n",
    "train_index = np.random.choice(data_size, int(data_size*train_ratio), replace=False)\n",
    "valid_index = np.setdiff1d(np.arange(data_size), train_index)\n",
    "\n",
    "np.savetxt('train.tsv',labeled_df.iloc[train_index].values, fmt=\"%s\\t%i\") \n",
    "np.savetxt('valid.tsv',labeled_df.iloc[valid_index].values, fmt=\"%s\\t%i\") \n",
    "\n",
    "print(\"Data is splitted into:\")\n",
    "print(\"Training data: {} records.\".format(len(train_index)))\n",
    "print(\"valid data: {} records.\".format(len(valid_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ヘッダの追加**\n",
    "解析するテキストを`review_body`、ポジティブかネガティブかを表すラベルを`star_rating`としてヘッダを与えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '1s/^/review_body\\tstar_rating\\n/' train.tsv\n",
    "!sed -i '1s/^/review_body\\tstar_rating\\n/' valid.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **学習の実行**\n",
    "### **学習データのアップロード**\n",
    "S3に学習データと検証データをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "s3_train_data = sess.upload_data(path='train.tsv', key_prefix='amazon-review-data-tsv')\n",
    "s3_valid_data = sess.upload_data(path='valid.tsv', key_prefix='amazon-review-data-tsv')\n",
    "print(\"Training data is uploaded to {}\".format(s3_train_data))\n",
    "print(\"Training data is uploaded to {}\".format(s3_valid_data))\n",
    "\n",
    "data_channels = {'train': s3_train_data, 'val': s3_valid_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **学習ジョブの実行**\n",
    "今回はGPUインスタンスであるml.p3.2xlargeを使用します。テストのため、epochは1とします。\n",
    "`src`ディレクトリにある`requirements.txt`、`train_and_deploy.py`、`EarlyStopping.py`が\n",
    "学習インスタンスへアップロードされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "estimator = PyTorch(entry_point='train_and_deploy.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=2,\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **デプロイ**\n",
    "学習したモデルをデプロイします。推論はCPUインスタンスで行うことができます。<br>\n",
    "最後に`!`記号が現れたらデプロイ完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **推論**\n",
    "実際の文を用いた推論を行います。\n",
    "response_hypoの値が1に近ければポジティブ、0に近ければネガティブです。\n",
    "\n",
    "Pytorchコンテナではserializer等がnumpyであるため、deploy後に以下のようにしてserializerを変更します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer\n",
    "from sagemaker.predictor import json_deserializer\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_hypo = predictor.predict(\"価格にもクオリティにも満足した。\")\n",
    "prediction_label = \"ポジティブ\" if response_hypo[0] >= 0.5 else \"ネガティブ\"\n",
    "print(\"label: {} (hypo value: {})\".format(prediction_label,response_hypo[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
