{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering with RAPIDS on SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このノートブックのインスタンスタイプの確認\n",
    "- このノートブックを実行するインスタンスにはml.t2.large（vCPU2, メモリ8GB）以上のインスタンスタイプを使用してください。ml.t2.medium（vCPU2, メモリ4GB）の場合、コンテナのビルドでメモリエラーが発生します。\n",
    "\n",
    "### pipとSageMakerの更新\n",
    "- このnotebookはSageMakerのVersion 2.X 以降で動作するため、SageMakerのVersionが2.0以降でない場合はUpgradeを行う必要があります。\n",
    "- 下のセルでは2020年11月時点での最新バージョンへ更新を行っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install  sagemaker==2.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するライブラリのImportとSageMaker, S3の変数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "# SageMakerのVersionが2.0以降であることを確認してください\n",
    "# もし、upgrade後もVersionが2.0以降に変わらない場合は、カーネルを再起動してください\n",
    "print('Current SageMaker Python sdk Version ={0}'.format(sagemaker.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = \"sagemaker/rapids-preprocess-demo\"\n",
    "input_prefix = prefix + \"/input/raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのダウンロードとAmazon Simple Storage Service（Amazon S3）へのアップロード\n",
    "ここで使用するデータセットは、Census-Income KDDデータセットです。これを使用して、国勢調査の回答者を表す行の収入が50,000ドルより大きいか小さいかを予測するための前処理や特徴量の作成を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "region = sagemaker_session.boto_region_name\n",
    "input_data = 's3://sagemaker-sample-data-{}/processing/census/census-income.csv'.format(region)\n",
    "!aws s3 cp $input_data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込みとターゲット変数の変換\n",
    "\n",
    "df = pd.read_csv('census-income.csv')\n",
    "print(df.income.value_counts())\n",
    "df['income'] = np.where(df['income'] == ' 50000+.', 1, 0) #ターゲット変数の(0, 1)変換\n",
    "print(df.income.value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データと検証用データへの分割\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['income'])\n",
    "\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVファイルの書き出し\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3へのアップロード\n",
    "input_train = sagemaker_session.upload_data(path='train.csv', bucket=bucket, key_prefix=input_prefix)\n",
    "input_validation = sagemaker_session.upload_data(path='validation.csv', bucket=bucket, key_prefix=input_prefix)\n",
    "\n",
    "print('TrainData is here: ', input_train)\n",
    "print('validationData is here: ', input_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dockerコンテナの作成\n",
    "containerディレクトリの下にあるDockerfileをビルドします。（実行には15分ほどかかります）\n",
    "\n",
    "さらに必要なライブラリがあれば、Dockerfile内に記述してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%cd container\n",
    "!docker build -t sagemaker-rapids-example .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository URIの設定\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "ecr_repository = 'sagemaker-rapids-example'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "rapids_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "print('Your repository URI is: ', rapids_repository_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon ECR repositoryを作成し、dockerコンテナをPUSHする\n",
    "上のセルのrepository URIの名前でECRにrepositoryを作成し、sagemaker-rapids-exampleコンテナをPUSHします\n",
    "（実行には10分前後かかります）\n",
    "\n",
    "一度コンテナイメージの作成からECRへのPUSHまでを行えば、以降同じコンテナイメージは上のセルで出力されるURIで使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $rapids_repository_uri\n",
    "!docker push $rapids_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Processing内で実行したいコードを記述します。\n",
    "\n",
    "コンテナの作成が完了したら、コンテナ上で実行したいスクリプトを作成します。今回は、RAPIDSのcuDFとcuMLを使用して、データの読み込みとカテゴリ変数に対するLabelEncoding, TargetEncodingを行い、処理後のファイルを出力した後、XGBoostでモデルの学習を行いたいと思います。\n",
    "\n",
    "XGBoostのモデルについても、例えば'/opt/ml/processing/output/model'といった名前のディレクトリを作成し、そこへモデルを出力することでS3への保存が可能です。\n",
    "\n",
    "下のセルを実行するとこのノートブックインスタンスと同じ階層にpreprocess.pyが生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cudf\n",
    "from cuml.preprocessing.LabelEncoder import LabelEncoder\n",
    "from cuml.preprocessing.TargetEncoder import TargetEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Get processor scrip arguments\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    script_args = dict(zip(args_iter, args_iter))\n",
    "    \n",
    "    TARGET_COL = script_args['TARGET_COL']\n",
    "    TE_COLS = [x.strip() for x in script_args['TE_COLS'].split(',')]\n",
    "    SMOOTH = float(script_args['SMOOTH'])\n",
    "    SPLIT = script_args['SPLIT']\n",
    "    FOLDS = int(script_args['FOLDS'])\n",
    "    \n",
    "    # Read train, validation data\n",
    "    train = cudf.read_csv('/opt/ml/processing/input/train/train.csv')\n",
    "    valid = cudf.read_csv('/opt/ml/processing/input/valid/validation.csv')\n",
    "    \n",
    "    start = time.time(); print('Creating Feature...')\n",
    "    \n",
    "    # Define categorical columns\n",
    "    catcols = [x for x in train.columns if x not in [TARGET_COL] and train[x].dtype == 'object']\n",
    "    \n",
    "    # Label encoding\n",
    "    for col in catcols:\n",
    "        train[col] = train[col].fillna('None')\n",
    "        valid[col] = valid[col].fillna('None')\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(cudf.concat([train[col], valid[col]]))\n",
    "        train[col] = lbl.transform(train[col])\n",
    "        valid[col] = lbl.transform(valid[col])\n",
    "    \n",
    "    # Target encoding\n",
    "    for col in TE_COLS:\n",
    "        out_col = f'{col}_TE'\n",
    "        encoder = TargetEncoder(n_folds=FOLDS, smooth=SMOOTH, split_method=SPLIT)\n",
    "        encoder.fit(train[col], train[TARGET_COL])\n",
    "        train[out_col] = encoder.transform(train[col])\n",
    "        valid[out_col] = encoder.transform(valid[col])\n",
    "        \n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "        \n",
    "    print(train.shape)\n",
    "    print(valid.shape)\n",
    "    print(train.head())\n",
    "    print(valid.head())\n",
    "        \n",
    "    # Create local output directories\n",
    "    try:\n",
    "        os.makedirs('/opt/ml/processing/output/train')\n",
    "        os.makedirs('/opt/ml/processing/output/valid')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Save data locally\n",
    "    train.to_csv('/opt/ml/processing/output/train/train.csv', index=False)\n",
    "    valid.to_csv('/opt/ml/processing/output/valid/validation.csv', index=False)\n",
    "        \n",
    "    # Train XGBoost model\n",
    "    print('XGBoost Version: ',xgb.__version__)\n",
    "    \n",
    "    xgb_parms = { \n",
    "    'max_depth':6, \n",
    "    'learning_rate':0.1, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':1.0, \n",
    "    'eval_metric':'auc',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'gpu_hist'\n",
    "    }\n",
    "    \n",
    "    NROUND = 100\n",
    "    VERBOSE_EVAL = 10\n",
    "    ESR = 10\n",
    "\n",
    "    start = time.time(); print('Creating DMatrix...')\n",
    "    dtrain = xgb.DMatrix(data=train.drop(TARGET_COL,axis=1),label=train[TARGET_COL])\n",
    "    dvalid = xgb.DMatrix(data=valid.drop(TARGET_COL,axis=1),label=valid[TARGET_COL])\n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "\n",
    "    start = time.time(); print('Training...')\n",
    "    model = xgb.train(xgb_parms, \n",
    "                      dtrain=dtrain,\n",
    "                      evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                      num_boost_round=NROUND,\n",
    "                      early_stopping_rounds=ESR,\n",
    "                      verbose_eval=VERBOSE_EVAL)\n",
    "    \n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "    \n",
    "    print('Finished running processing job')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processorの定義\n",
    "ScriptProcessorクラスのimage_uriの引数に、先ほど作成したRAPIDSが実行できるコンテナリポジトリのURIを渡し、インスタンスタイプにGPU（今回はml.p3.2xlarge）を指定し、任意の名前のオブジェクトを作成します。\n",
    "\n",
    "- ml.p3.2xlargeはNVIDIA Tesla V100が1枚搭載されているインスタンスですが、RAPIDSはマルチGPUにも対応しているため、例えば、NVIDIA Tesla V100が4枚搭載されたp3.8xlargeを使用してpreprocess.pyの中の処理をマルチGPUが実行できるように書き換えることでさらに大規模データへの対応や高速化が実現できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "rapids_processor = ScriptProcessor(\n",
    "    role=role, \n",
    "    image_uri=rapids_repository_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.p3.2xlarge\", # use GPU Instance\n",
    "    volume_size_in_gb=30, \n",
    "    volume_kms_key=None, \n",
    "    output_kms_key=None, \n",
    "    max_runtime_in_seconds=86400, # the default value is 24 hours(60*60*24)\n",
    "    base_job_name=\"rapids-preprocessor\",\n",
    "    sagemaker_session=None, \n",
    "    env=None, \n",
    "    tags=None, \n",
    "    network_config=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理・特徴量作成したファイルの出力先の定義\n",
    "output_s3_path = 's3://' + bucket + '/' + prefix + '/input/dataset/'\n",
    "print('File Output path: ', output_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processorの実行\n",
    "\n",
    "下のセルを実行するとpreprocess.pyはS3のsagemaker-{region}-{accountID}の直下に保存されます。\n",
    "\n",
    "- codeには先ほど作成したpreprocess.pyを指定します。\n",
    "- inputsのsourceには入力ファイルがあるS3のPATH、destinationにはコンテナ上でそのファイルを置く場所を指定します。\n",
    "- outputsのsourceには出力したいファイルのコンテナ上の場所、destinationには出力したいファイルを置くS3のPATHを指定します。\n",
    "- argumentsには、任意でスクリプト（preprocess.py）内に渡す引数を設定できます。今回はTargetEncodingに使用するターゲット変数の名前やTargetEncodingする変数名、ハイパーパラメータなどを設定していますが、これらはpreprocess.py内に直接記述することも可能です。\n",
    "\n",
    "また、inputs、outputsは必ずしも設定する必要はなく、preprocess.py内でインターネット上からファイルを取得して、何かしらの処理をして出力することもできますし、入力ファイルから機械学習モデルを学習し、ひとまず精度を確認したり、学習用インスタンスで実行するためのコードを開発するといったことにも使えます（このnotebookでは前処理・特徴量作成のあとにXGBoostモデルの学習も行っています）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rapids_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_train, destination='/opt/ml/processing/input/train'), \n",
    "        ProcessingInput(source=input_validation, destination='/opt/ml/processing/input/valid')\n",
    "    ], \n",
    "    outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/train', destination=output_s3_path),\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/valid', destination=output_s3_path)\n",
    "    ],\n",
    "    arguments=[\n",
    "        'TARGET_COL', 'income',\n",
    "        'TE_COLS', 'class of worker, education, major industry code',\n",
    "        'SMOOTH', '0.001',\n",
    "        'SPLIT', 'interleaved',\n",
    "        'FOLDS', '5'\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True,\n",
    "    job_name=None,\n",
    "    experiment_config=None, \n",
    "    kms_key=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 処理結果の確認\n",
    "\n",
    "s3のバケットからファイルを呼び出し、処理後のファイルを確認してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list=s3.list_objects_v2(Bucket=bucket, Prefix=prefix + '/input/dataset/')\n",
    "\n",
    "file=[]\n",
    "for contents in obj_list['Contents']:\n",
    "    file.append(contents['Key'])\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file[file.index(prefix + '/input/dataset/train.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# oldの特徴量データを読み込む\n",
    "response = s3.get_object(Bucket=bucket, Key=file_path)\n",
    "response_body = response[\"Body\"].read()\n",
    "train_df = pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ変数がLabel encodingされていることと指定したカテゴリ変数がTarget encodingされていることを確認できました。\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 後片付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker ProcessingはJobが完了するとインスタンスが自動的に停止し、削除されます。\n",
    "\n",
    "一方、このノートブックインスタンスについてはSageMakerのコンソールから、手動で停止しない限り、料金が継続的にかかってしまいます。\n",
    "\n",
    "このノートブックの実行が終わったらインスタンスの停止と削除をお願いいたします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
