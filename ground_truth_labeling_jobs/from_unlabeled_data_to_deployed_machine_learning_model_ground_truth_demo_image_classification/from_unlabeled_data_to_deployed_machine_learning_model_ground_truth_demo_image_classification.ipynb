{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラベルのないデータから機械学習のモデルを構築する: 画像分類における SageMaker Ground Truth のデモ\n",
    "<!-- From Unlabeled Data to a Deployed Machine Learning Model: A SageMaker Ground Truth Demonstration for Image Classification\n",
    " -->\n",
    "1. [イントロダクション](#イントロダクション)\n",
    "2. [Ground Truth ラベリングジョブの実行 (目安の時間: 3 時間)](#Ground-Truth-ラベリングジョブの実行)\n",
    "    1. [データの準備](#データの準備)\n",
    "    2. [カテゴリを特定する](#カテゴリを特定する)\n",
    "    3. [指示書テンプレートの作成](#指示書テンプレートの作成)\n",
    "    4. [タスクをテストするためのプライベートチームを作成する [オプション]](#タスクをテストするためのプライベートチームを作成する-[オプション])\n",
    "    5. [ラベリングジョブで使用する構築済み Lambda 関数を定義する](#ラベリングジョブで使用する構築済み-Lambda-関数を定義する)\n",
    "    6. [Ground Truth ジョブリクエストを送信する](#Ground-Truth-ジョブリクエストを送信する)\n",
    "        1. [Vプライベートチームでタスクを検証する [オプション]](#プライベートチームでタスクを検証する-[オプション])\n",
    "    7. [ジョブの進捗状況を監視する](#ジョブの進捗状況を監視する)\n",
    "3. [Ground Truth ラベリングジョブの結果を分析する (目安の時間: 20 分)](#Ground-Truth-ラベリングジョブの結果を分析する)\n",
    "    1. [出力マニフェストファイルの結果を後処理する](#出力マニフェストファイルの結果を後処理する)\n",
    "    2. [クラスに対するヒストグラムを作成する](#クラスに対するヒストグラムを作成する)\n",
    "    3. [アノテーションされた画像を表示する](#アノテーションされた画像を表示する)\n",
    "        1. [数枚の出力サンプルを表示する](#数枚の出力サンプルを表示する)\n",
    "        2. [すべての結果を表示する](#すべての結果を表示する)\n",
    "4. [Ground Truth の結果を、既知の事前にラベル付けされたデータと比較する (目安の時間: 5 分)](#Ground-Truth-の結果を、既知の事前にラベル付けされたデータと比較する)\n",
    "    1. [精度を計算する](#精度を計算する)\n",
    "    2. [正誤のアノテーションをプロットする](#正誤のアノテーションをプロットする)\n",
    "5. [Ground Truth を使った画像分類器のトレーニング (目安の時間: 15 分)](#Ground-Truth-を使った画像分類器のトレーニング)\n",
    "6. [モデルをデプロイする (目安の時間: 20 分)](#モデルをデプロイする)\n",
    "    1. [モデルの生成](#モデルの生成)\n",
    "    2. [バッチ変換](#バッチ変換)\n",
    "    3. [リアルタイム推論](#リアルタイム推論)\n",
    "        1. [エンドポイント設定を作成する](#エンドポイント設定を作成する)\n",
    "        2. [エンドポイントを作成する](#エンドポイントを作成する)\n",
    "        3. [推論を実行する](#推論を実行する)\n",
    "7. [まとめ](#まとめ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "このサンプルノートブックでは、SageMaker Ground Truth の機能を end-to-end のワークフローを通して体験することができます。まずラベルがついていない画像のデータセットからはじめ、SageMaker Ground Truth を使ってラベルを取得し、ラベリングジョブの結果を解析し、画像分類機の学習を行い、できたモデルをホストし、そして最後にそれを使った推論を行います。始める前に、ご自身がワークフローに慣れるために、まずは AWS コンソールで Ground Truth のラベリングジョブを始めることを強く推奨します。AWS コンソールは API に比べて柔軟性が低いものになりますが、使用は簡単です。\n",
    "\n",
    "#### 費用とランタイム\n",
    "このデモは２つのモードで実行できます：\n",
    "1. 次のセルで`RUN_FULL_AL_DEMO = True`とすることで、1000枚の画像のラベル付けを行います。費用は現在の [Ground Truth の料金体系](https://aws.amazon.com/jp/sagemaker/data-labeling/pricing/)だと、およそ \\$80 かかります。費用を削減するために、このデモでは、Ground Truth のラベル付けの自動化の機能を使用します。この機能はコンピュータビジョンにより人間による回答を学習し、安い料金で最も簡単な画像のラベルを自動で生成します。すべてのランタイムを実行するにはおよそ 4 時間かかります。\n",
    "1. 次のセルで`RUN_FULL_AL_DEMO = False`とすることで、100枚だけの画像のラベル付けを行います。費用は \\$8 ほどです。**Ground Truth のラベル付けの自動化の機能は、1000枚以上の画像のデータセットのみに対して実行されるため、デモのこの安価な版では使用しません。下記で解析した結果の図はもしかしたら違和感があるように見えるかもしれませんが、人間による100枚の画像でも良い結果を見ることができます。**\n",
    "\n",
    "#### 前提条件\n",
    "各セルを１つずつ実行するだけで、このノートブックは実行できます。実際に何をしているかを理解するためには、下記のことが必要になります：\n",
    "* 書き込み可能な S3 バケット -- 次のセルにバケット名を記載してください。バケットは SageMaker ノートブックインスタンスと同じリージョンにいなければなりません。また、`EXP_NAME` は任意の S3 プリフィクスに変更できます。この試行に関連するすべてのファイルは指定されたバケットのプリフィクスに格納されます。\n",
    "* このデモで使う S3 バケットは、CORS ポリシーが付与されている必要があります。この条件やどのように CORS ポリシーを S3 バケットに付与するかについては、[CORS Permission Requirement](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html) をご確認ください。\n",
    "* Python と [numpy](http://www.numpy.org/) の知識\n",
    "* [AWS S3](https://docs.aws.amazon.com/s3/index.html) の基礎的な知識\n",
    "* [AWS Sagemaker](https://aws.amazon.com/sagemaker/) の基礎的な理解\n",
    "* [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) の基礎的な知識 -- このノートブックを実行している AWS アカウントへのアクセスのための認証情報をセットアップしてください。それにより SageMaker Jupyter ノートブックインスタンスの枠を超えて実行することができます。\n",
    "\n",
    "このノートブックは SageMaker ノートブックインスタンスでのみテストされています。指定されているランタイムはおおよそのものであり、テストでは `ml.m4.xlarge` インスタンスを使っています。しかし、初めに次のセルを SageMaker で実行し、ローカルにコピーしたノートブックに `role` の文字列をコピーすることで、ローカルのインスタンスでも実行可能です。\n",
    "\n",
    "注：このノートブックは作業用のディレクトリにサブディレクトリを作成・削除します。実行前に、このノートブックを自身のディレクトリに配置することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import boto3\n",
    "import sagemaker\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "BUCKET = \"<< YOUR S3 BUCKET NAME >>\"\n",
    "assert BUCKET != \"<< YOUR S3 BUCKET NAME >>\", \"Please provide a custom S3 bucket name.\"\n",
    "EXP_NAME = \"ground-truth-ic-demo\"  # 有効な任意の S3 prefix\n",
    "RUN_FULL_AL_DEMO = False  # 上述の '費用とランタイム' を確認してください!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブックが S3 バケットと同じリージョンにいることを確認してください。\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)[\"ResponseMetadata\"][\"HTTPHeaders\"][\n",
    "    \"x-amz-bucket-region\"\n",
    "]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), \"You S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth ラベリングジョブの実行\n",
    "**このセクションは完了に3時間程度かかります。**\n",
    "\n",
    "まずはじめにラベリングジョブを実行します。このためには複数のステップを実行します。ラベル付けを行いたいデータを収集し、可能性のあるカテゴリを特定し、指示書を作成し、ラベリングジョブの仕様を記述します。加えて、パブリックワークフォースにジョブを公開する前に、プライベートワークフォースを利用して、（無料の）擬似のジョブを実行することを強く推奨します。このノートブックではオプションとして、その手順を説明します。プライベートワークフォースなしでも、ラベリングジョブの完了までに３時間ほどかかります。しかし、これはパブリックのアノテーションワークフォースの利用可能状況によって変わります。\n",
    "\n",
    "## データの準備\n",
    "まず [Google Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html) から画像とラベルの一部をダウンロードします。これらのラベルは[正当性が慎重に保証されています](https://storage.googleapis.com/openimages/web/factsfigures.html)。あとの方で、このラベルと Ground Truth のアノテーションの結果を比較します。今回のデータセットの画像は次のカテゴリーを含みます：\n",
    "\n",
    "* 音楽楽器（Musical Instrument、500枚)\n",
    "* フルーツ（Fruit、370枚)\n",
    "* チーター（Cheetah、50枚)\n",
    "* 虎（Tiger、40枚)\n",
    "* 雪だるま（Snowman、40枚)\n",
    "\n",
    "`RUN_FULL_AL_DEMO = False` を選択した場合、このデータセットのうち100枚の画像が使われます。このデータセットは、面白みのある画像を含んでいるので、アノテーターにとっても楽しいものになるはずです。アノテーターにはどんな画像でも望むもののラベル付けを依頼することができます（ただし、アダルトコンテンツを含まない場合に限ります。この場合、ラベリングジョブのリクエストをそのジョブ用の方法に調整する必要があります。詳しくは Ground Truth の開発者ガイドをご覧ください）。\n",
    "\n",
    "これらの画像は、ローカルの `BUCKET` にコピーされ、対応する*入力マニフェスト*を生成します。この入力マニフェストは、Ground Truth にアノテーションを行わせたい画像の S3 内の位置の整形化リストです。これを S3 `BUCKET` にアップロードします。\n",
    "\n",
    "#### Open Images Dataset V4 に関する公開：\n",
    "Open Images Dataset V4 は Google Inc.　によって作成されています. 画像や付随しているアノテーションを修正していません。画像とアノテーションについては、 [こちら](https://storage.googleapis.com/openimages/web/download.html)から取得できます。アノテーションは [CC BY 4.0](https://creativecommons.org/licenses/by/2.0/) ライセンスのもと、Google Inc. によってライセンスを提供されています。 画像は [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/) ライセンスを持つとしてリストに載っています。次の論文では. Open Images V4 について、データ収集やアノテーションからデータの統計情報やそれに基づいて学習されたモデルの評価まで、深く説明しています。\n",
    "\n",
    "A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, T. Duerig, and V. Ferrari.\n",
    "*The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale.* arXiv:1811.00982, 2018. ([PDFのリンク](https://arxiv.org/abs/1811.00982))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open Images のアノテーションをダウンロードし、処理する。\n",
    "!wget https://storage.googleapis.com/openimages/2018_04/test/test-annotations-human-imagelabels-boxable.csv -O openimgs-annotations.csv\n",
    "with open(\"openimgs-annotations.csv\", \"r\") as f:\n",
    "    all_labels = [line.strip().split(\",\") for line in f.readlines()]\n",
    "\n",
    "# 各カテゴリのイメージ ID を取得\n",
    "ims = {}\n",
    "ims[\"Musical Instrument\"] = [\n",
    "    label[0] for label in all_labels if (label[2] == \"/m/04szw\" and label[3] == \"1\")\n",
    "][:500]\n",
    "ims[\"Fruit\"] = [label[0] for label in all_labels if (label[2] == \"/m/02xwb\" and label[3] == \"1\")][\n",
    "    :371\n",
    "]\n",
    "ims[\"Fruit\"].remove(\n",
    "    \"02a54f6864478101\"\n",
    ")  # この画像は個人情報を含むため、データセットから削除します。\n",
    "ims[\"Cheetah\"] = [label[0] for label in all_labels if (label[2] == \"/m/0cd4d\" and label[3] == \"1\")][\n",
    "    :50\n",
    "]\n",
    "ims[\"Tiger\"] = [label[0] for label in all_labels if (label[2] == \"/m/07dm6\" and label[3] == \"1\")][\n",
    "    :40\n",
    "]\n",
    "ims[\"Snowman\"] = [\n",
    "    label[0] for label in all_labels if (label[2] == \"/m/0152hh\" and label[3] == \"1\")\n",
    "][:40]\n",
    "num_classes = len(ims)\n",
    "\n",
    "# 短縮版のデモの場合、各クラスの最初の1/10を使用する。\n",
    "for key in ims.keys():\n",
    "    if RUN_FULL_AL_DEMO is False:\n",
    "        ims[key] = set(ims[key][: int(len(ims[key]) / 10)])\n",
    "    else:\n",
    "        ims[key] = set(ims[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルのバケットに画像をコピーする。\n",
    "s3 = boto3.client(\"s3\")\n",
    "for img_id, img in enumerate(itertools.chain.from_iterable(ims.values())):\n",
    "    if (img_id + 1) % 10 == 0:\n",
    "        print(\"Copying image {} / {}\".format((img_id + 1), 1000))\n",
    "    copy_source = {\"Bucket\": \"open-images-dataset\", \"Key\": \"test/{}.jpg\".format(img)}\n",
    "    s3.copy(copy_source, BUCKET, \"{}/images/{}.jpg\".format(EXP_NAME, img))\n",
    "\n",
    "# インプットマニフェストを作成してアップロードする。\n",
    "manifest_name = \"input.manifest\"\n",
    "with open(manifest_name, \"w\") as f:\n",
    "    for img in itertools.chain.from_iterable(ims.values()):\n",
    "        img_path = \"s3://{}/{}/images/{}.jpg\".format(BUCKET, EXP_NAME, img)\n",
    "        f.write('{\"source-ref\": \"' + img_path + '\"}\\n')\n",
    "s3.upload_file(manifest_name, BUCKET, EXP_NAME + \"/\" + manifest_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のセルを実行後、[S3 コンソール](https://console.aws.amazon.com/s3/) から `s3://BUCKET/EXP_NAME/images` に行くことができ、1000枚分の画像を確認できます。これらの画像を詳しく見ることをお勧めします！ AWS CLI を使うことで、ローカルのマシンにすべての画像をダウンロードすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリを特定する\n",
    "\n",
    "画像分類のラベリングジョブを実行するためには、アノテーターが選択するための分類を決める必要があります。この場合は、`[\"Musical Instrument\", \"Fruit\", \"Cheetah\", \"Tiger\", \"Snowman\"]`のリストになります。ジョブの中では10個までクラスを指定することができます。これらのクラスは明確で具体的であることをお勧めします。カテゴリは相互に排他的で、各画像には１つの正しい画像が特定されるべきです。加えて、できる限りタスクを*客観的*にするように注意する必要があります。ただし、もちろん、主観的なラベルを得ることを意図している場合は、その限りではありません。\n",
    "* よりカテゴリのリストの例: `[\"Human\", \"No Human\"]`, `[\"Golden Retriever\", \"Labrador\", \"English Bulldog\", \"German Shepherd\"]`, `[\"Car\", \"Train\", \"Ship\", \"Pedestrian\"]`.\n",
    "* 悪いカテゴリのリストの例: `[\"Prominent object\", \"Not prominent\"]` (prominent=目立つ、の意味が明確でない), `[\"Beautiful\", \"Ugly\"]` (主観的), `[\"Dog\", \"Animal\", \"Car\"]` (相互に排他的でない). \n",
    "\n",
    "Ground Truth を使う場合、このリストは .json ファイルに変換され、S3 `BUCKET` にアップロードする必要があります。\n",
    "\n",
    "*注：テンプレート内のラベルやクラスの順序は出力マニフェストで出てくるクラスのインデックスを規定するものになります（0から始まるインデックスです）。つまり、テンプレートで２番目に現れるクラスは、出力では \"1\" のクラスに対応します。このデモの終わりに、モデルの学習と推論を行いますが、このクラスの順序は結果を解釈するための手段となります。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LIST = list(ims.keys())\n",
    "print(\"Label space is {}\".format(CLASS_LIST))\n",
    "\n",
    "json_body = {\"labels\": [{\"label\": label} for label in CLASS_LIST]}\n",
    "with open(\"class_labels.json\", \"w\") as f:\n",
    "    json.dump(json_body, f)\n",
    "\n",
    "s3.upload_file(\"class_labels.json\", BUCKET, EXP_NAME + \"/class_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行すると、`s3://BUCKET/EXP_NAME/`に`class_labels.json`を確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指示書テンプレートの作成\n",
    "一部またはすべての画像は人間のアノテーターによってアノテーションされます。したがって、アノテーターがあなたの欲しいようにアノテーションできるように良い指示書を提供することは**不可欠**です。良い指示書とは：\n",
    "1. 簡潔。言語や本文での指示は２文に制限し、明確な画像に注力した方が良いです。\n",
    "2. 画像。画像分類の場合は、指示書の一部として、各クラスごとのラベル付けされが画像を含めると良いです。\n",
    "\n",
    "AWS コンソールをお使いの場合、Ground Truth ではビジュアルウィザードを使って作業書を作成することができます。API をお使いの場合、作業書のためには HTML テンプレートを作成する必要があります。下記では、とても簡単ですが有用なテンプレートを用意しており、あなたのS3 バケットにアップロードします。\n",
    "\n",
    "注：（今回のように）画像をテンプレートで使用する場合、それらの画像はパブリックにアクセス可能である必要があります。S3 コンソールから S3 バケットないのファイルにパブリックなアクセスを許可することができます。詳細は、[S3 の開発者ガイド](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-object-permissions.html)をご確認ください。\n",
    "\n",
    "#### 指示書をテストする\n",
    "破綻している指示書を作成するのは非常に簡単です。これによってラベリングジョブは失敗するかもしれません。もしかしたら、（アノテーターが何をすれば良いのかわからない時や、指示書が完全に間違っているときに）意味のない結果で完了することになるかもしれません。したがって、タスクが正しいかを次の２つの方法で確認することを*強く推奨します*：\n",
    "1. 次のセルは、`instructions.template` というファイルを生成して、S3 にアップロードします。また、ローカルのブラウザで開くことのできる`instructions.html` というファイルを生成します。ぜひブラウザで開いてみて、生成された web ページを確認してください。あなたが（実際のアノテーションされる画像が表示されていないことを除いて）アノテーターに見えて欲しいようになっているべきです。\n",
    "2. プライベートワークフォースを使用してジョブを実行してください。これは擬似的にラベリングジョブを実行する方法です。この方法については、[プライベートチームでタスクを検証する [オプション]](#プライベートチームでタスクを検証する-[オプション])の中で説明します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_examples = [\n",
    "    \"https://s3.amazonaws.com/open-images-dataset/test/{}\".format(img_id)\n",
    "    for img_id in [\n",
    "        \"0634825fc1dcc96b.jpg\",\n",
    "        \"0415b6a36f3381ed.jpg\",\n",
    "        \"8582cc08068e2d0f.jpg\",\n",
    "        \"8728e9fa662a8921.jpg\",\n",
    "        \"926d31e8cde9055e.jpg\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def make_template(test_template=False, save_fname=\"instructions.template\"):\n",
    "    template = r\"\"\"<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "    <crowd-form>\n",
    "      <crowd-image-classifier\n",
    "        name=\"crowd-image-classifier\"\n",
    "        src=\"{{{{ task.input.taskObject | grant_read_access }}}}\"\n",
    "        header=\"Dear Annotator, please tell me what you can see in the image. Thank you!\"\n",
    "        categories=\"{categories_str}\"\n",
    "      >\n",
    "        <full-instructions header=\"Image classification instructions\">\n",
    "        </full-instructions>\n",
    "\n",
    "        <short-instructions>\n",
    "          <p>Dear Annotator, please tell me whether what you can see in the image. Thank you!</p>\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Musical Instrument\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Fruit\".</p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Cheetah\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Tiger\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Snowman\". </p>\n",
    "\n",
    "        </short-instructions>\n",
    "\n",
    "      </crowd-image-classifier>\n",
    "    </crowd-form>\"\"\".format(\n",
    "        *img_examples,\n",
    "        categories_str=str(CLASS_LIST)\n",
    "        if test_template\n",
    "        else \"{{ task.input.labels | to_json | escape }}\",\n",
    "    )\n",
    "\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "    if test_template is False:\n",
    "        print(template)\n",
    "\n",
    "\n",
    "make_template(test_template=True, save_fname=\"instructions.html\")\n",
    "make_template(test_template=False, save_fname=\"instructions.template\")\n",
    "s3.upload_file(\"instructions.template\", BUCKET, EXP_NAME + \"/instructions.template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンプレートは `s3://BUCKET/EXP_NAME/instructions.template`で確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## タスクをテストするためのプライベートチームを作成する [オプション]\n",
    "このステップでは AWS コンソールを使用します。しかし、特に独自のデータセットやラベルセット、テンプレートでタスクを作成するときに、このステップを行うことを**強く推奨します**。\n",
    "\n",
    "ここでは`プライベートワークスチーム` を作り、ユーザーを一人（あなた）追加します。その後、Ground Truth のジョブリクエストの API を修正して、タスクをワークフォースに送信します。これにより、パブリックアノテーターが見る内容と同一のものをご自身で確認することができます。あるいは、全てのデータセットを自身でアノテーションすることもできます。\n",
    "\n",
    "プライベートチームを作成する方法は、以下の通りです：\n",
    "1. `AWS Console > Amazon SageMaker > Ground Truth > ラベリングワークフォース` に行きます。\n",
    "1. `プライペート`をクリックし、\"プライベートチーム\"の下の`プライベートチームを作成`をクリックします。 \n",
    "1. `チーム名`にプライベートチームの適当な名前を入力します。\n",
    "1. `プライベートチームを作成`をクリックします。\n",
    "1. 画面が `AWS Console > Amazon SageMaker > Ground Truth > ラベリングワークフォース` に戻ります。新しく作成したチームが、\"プライベートチーム\"の下に表示されます。`arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name`のような`ARN`が表示されるので、それを下のセルにコピーします。\n",
    "1. \"ワーカー\"の下の`新しいワーカーを招待`をクリックします。\n",
    "1. `E メールアドレス`の欄に、自身のメールアドレスを入力します。\n",
    "1. `新しいワーカーを招待`をクリックします。\n",
    "1. `no-reply@verificationemail.com` からワークフォースのユーザー名、パスワードとログイン URL が入った E メールが送信されます。メール本文中の URL をクリックし、ユーザー名とパスワードでログインします (新しいパスワードの生成を求められます)。\n",
    "1. `AWS Console > Amazon SageMaker > Ground Truth > ラベリングワークフォース` の画面で、\"プライベートチーム\"の下の作成したチーム名をクリックします。\n",
    "1. `ワーカー`のタブを選択し、`チームにワーカーを追加`をクリックします。\n",
    "1. 登録した E メールアドレスを選択し、`チームにワーカーを追加`をクリックします。\n",
    "\n",
    "これで完了です！9 で表示したページが、あなたのプライベートワーカーのインターフェースです。下記の[プライベートチームでタスクを検証する [オプション]](#プライベートチームでタスクを検証する-[オプション]) の中でタスクを作成したとき、タスクはこの画面に現れます。\"新しいワーカーを招待\"ボタンをクリックすることで、同僚をラベリングジョブに招待することができます。\n",
    "\n",
    "[SageMaker Ground Truth 開発者ガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-private.html) にはプライベートワークフォースの管理方法について、詳細なドキュメントがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_workteam_arn = \"<< YOUR PRIVATE WORKTEAM ARN >>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベリングジョブで使用する構築済み Lambda 関数を定義する\n",
    "ジョブリクエストを送る前に、次の４つの主要な要素について ARN を定義する必要があります：1) ワークフォースのチーム、2)　アノテーション用統合 Lambda 関数、3) ラベリング前タスク用の Lambda 関数、4) 自動アノテーションを行う機械学習アルゴリズム。これらの関数は、リージョン名、AWS サービスアカウントナンバーの文字列で定義されており、サポートされているリージョンでノートブックを実行できるように下記でマッピングを定義します。\n",
    "\n",
    "利用可能な ARN  については公式のドキュメントを参照してください：\n",
    "* [SageMaker 開発者ガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-public.html)：ワークフォースチームの ARN の定義を記載しています。パブリックワークフォースを使用する場合、利用可能な選択肢は１つしかありません。プライベートチームを使用する場合、各チームの対応する ARN を確認してください。\n",
    "* [SageMaker API リファレンス - HumanTaskConfig](https://docs.aws.amazon.com/sagemaker/latest/dg/API_HumanTaskConfig.html#SageMaker-Type-HumanTaskConfig-PreHumanTaskLambdaArn) ：他のワークフローで利用可能な、人間によるラベリングタスクの前に実行される Lambda 関数の ARN を記しています。\n",
    "* [SageMaker API リファレンス - AnnotationConsolidationConfig](https://docs.aws.amazon.com/sagemaker/latest/dg/API_AnnotationConsolidationConfig.html#SageMaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn)：他のワークフローで利用可能な、アノテーション用統合 Lambda 関数の ARN を記しています。\n",
    "* [SageMaker API リファレンス - LabelingJobAlgorithmsConfig](https://docs.aws.amazon.com/sagemaker/latest/dg/API_LabelingJobAlgorithmsConfig.html#SageMaker-Type-LabelingJobAlgorithmsConfig-LabelingJobAlgorithmSpecificationArn)：他のワークフローで利用可能な、自動ラベリングアルゴリズムの ARN を記しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分類ジョブの実行に必要なリソースの ARN を指定します.\n",
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\",\n",
    "    \"ap-northeast-1\": \"477331159723\",\n",
    "}\n",
    "\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-ImageMultiClass\".format(\n",
    "    region, ac_arn_map[region]\n",
    ")\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-ImageMultiClass\".format(region, ac_arn_map[region])\n",
    "labeling_algorithm_specification_arn = \"arn:aws:sagemaker:{}:027400017018:labeling-job-algorithm-specification/image-classification\".format(\n",
    "    region\n",
    ")\n",
    "workteam_arn = \"arn:aws:sagemaker:{}:394669845002:workteam/public-crowd/default\".format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth ジョブリクエストを送信する\n",
    "下記の API でリクエストを送信することで、Ground Truth ジョブを開始できます。リクエストはアノテーションタスクの全ての設定を含み、AWS コンソールではデフォルト値に固定されていたジョブの細かな詳細も変更することができます。リクエストを生成するためのパラメータの詳細については、[SageMaker Ground Truth の API リファレンス](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateLabelingJob.html)に記載されています。\n",
    "\n",
    "リクエストを送信後、AWS コンソールの`Amazon SageMaker > Ground Truth > ラベリングジョブ`でジョブを見ることができます。ジョブの進捗状況もここで確認できます。ジョブの完了には数時間かかります。ジョブが大きい場合（例えば、10万枚の画像がある場合）、速度と費用に対する自動ラベリングの効果が大きくなります。\n",
    "\n",
    "### プライベートチームでタスクを検証する [オプション] \n",
    "[プライベートチームを作成する](#タスクをテストするためのプライベートチームを作成する-[オプション])の手順に従った場合、最初にタスクが期待通りかを確認することができます。\n",
    "1. 下記のセルで `VERIFY_USING_PRIVATE_WORKFORCE` を `True` にする。\n",
    "2. 下記の２つのセルを実行します。これでタスクが定義され、プライベートワークフォース（あなた）に送信されます。\n",
    "3. 数分後、プライベートワークフォースのインターフェースでタスクを見ることができます（[プライベートチームを作成する](#タスクをテストするためのプライベートチームを作成する-[オプション])も参照）。タスクが期待通りに表示されることを確認してください。\n",
    "4. 期待通りであれば、`VERIFY_USING_PRIVATE_WORKFORCE` を `False` にして、下記のセルを実際のタスクのために実行してください！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFY_USING_PRIVATE_WORKFORCE = True\n",
    "USE_AUTO_LABELING = True\n",
    "\n",
    "task_description = \"What do you see: a {}?\".format(\" a \".join(CLASS_LIST))\n",
    "task_keywords = [\"image\", \"classification\", \"humans\"]\n",
    "task_title = task_description\n",
    "job_name = \"ground-truth-demo-\" + str(int(time.time()))\n",
    "\n",
    "human_task_config = {\n",
    "    \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "    },\n",
    "    \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "    \"MaxConcurrentTaskCount\": 200,  # 一度に 200 枚の画像がワークフォースのチームに送られる\n",
    "    \"NumberOfHumanWorkersPerDataObject\": 1,  # 1 人の異なるワーカーが各画像のラベル付けに必要\n",
    "    \"TaskAvailabilityLifetimeInSeconds\": 21600,  # すべてのタスクを 6 時間以内に完了しなければいけない\n",
    "    \"TaskDescription\": task_description,\n",
    "    \"TaskKeywords\": task_keywords,\n",
    "    \"TaskTimeLimitInSeconds\": 300,  # 各画像を 5 分以内にラベル付けしなければいけない\n",
    "    \"TaskTitle\": task_title,\n",
    "    \"UiConfig\": {\n",
    "        \"UiTemplateS3Uri\": \"s3://{}/{}/instructions.template\".format(BUCKET, EXP_NAME),\n",
    "    },\n",
    "}\n",
    "\n",
    "if not VERIFY_USING_PRIVATE_WORKFORCE:\n",
    "    human_task_config[\"PublicWorkforceTaskPrice\"] = {\n",
    "        \"AmountInUsd\": {\n",
    "            \"Dollars\": 0,\n",
    "            \"Cents\": 1,\n",
    "            \"TenthFractionsOfACent\": 2,\n",
    "        }\n",
    "    }\n",
    "    human_task_config[\"WorkteamArn\"] = workteam_arn\n",
    "else:\n",
    "    human_task_config[\"WorkteamArn\"] = private_workteam_arn\n",
    "\n",
    "ground_truth_request = {\n",
    "    \"InputConfig\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"ManifestS3Uri\": \"s3://{}/{}/{}\".format(BUCKET, EXP_NAME, manifest_name),\n",
    "            }\n",
    "        },\n",
    "        \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\"FreeOfPersonallyIdentifiableInformation\", \"FreeOfAdultContent\"]\n",
    "        },\n",
    "    },\n",
    "    \"OutputConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/output/\".format(BUCKET, EXP_NAME),\n",
    "    },\n",
    "    \"HumanTaskConfig\": human_task_config,\n",
    "    \"LabelingJobName\": job_name,\n",
    "    \"RoleArn\": role,\n",
    "    \"LabelAttributeName\": \"category\",\n",
    "    \"LabelCategoryConfigS3Uri\": \"s3://{}/{}/class_labels.json\".format(BUCKET, EXP_NAME),\n",
    "}\n",
    "\n",
    "if USE_AUTO_LABELING and RUN_FULL_AL_DEMO:\n",
    "    ground_truth_request[\"LabelingJobAlgorithmsConfig\"] = {\n",
    "        \"LabelingJobAlgorithmSpecificationArn\": labeling_algorithm_specification_arn\n",
    "    }\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_client.create_labeling_job(**ground_truth_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ジョブの進捗状況を監視する\n",
    "Ground Truth のジョブは完了までに数時間かかります（データセットが1万枚以上だと、さらに長くかかります！）。ジョブの進捗を確認する１つの方法は AWS コンソールを使用することです。このノートブックでは、Ground Truth の出力ファイルと Cloud Watch logs を使った監視を行います。次の２つのセルを使って、繰り返し確認することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルは繰り返し実行できます。`describe_labelging_job`リクエストを送ることで、ジョブが完了したかどうかを表示します。完了したら、'LabelingJobStatus'が 'Completed'と表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.describe_labeling_job(LabelingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルは、ジョブに関するこれまでの状態をより詳細に抽出します。何回も繰り返すことができます。ここでは、下記の情報が表示されます：\n",
    "\n",
    "* ラベリングジョブの各反復において、人間と機械それぞれがアノテー小んした画像の枚数を各カテゴリごとに表示します。\n",
    "* Ground Truth により起動されたニューラルネットワークのトレーニングジョブの学習曲線 **(`RUN_FULL_AL_DEMO=True`で実行した場合のみ)**。\n",
    "* 人間と機械によりアノテーションしたラベルの料金\n",
    "\n",
    "料金体系を理解するためには、[料金ページ](https://aws.amazon.com/sagemaker/groundtruth/pricing/) を十分にご確認ください. 今回の場合、人間によるラベル付けは 1 回ごとに`$0.08 + 3 * $0.012 = $0.116` かかり、自動でのラベル付けは 1 回ごとに `$0.08`かかります。他にも、自動ラベリングの間、ニューラルネットの学習と推論に SageMaker インスタンスを使用する少額のコストがかかりますが、他のコストに比べればわずかなものです。\n",
    "\n",
    "`RUN_FULL_AL_DEMO==True`とした場合、 ステップを複数回繰り返すことでジョブが実行されます。\n",
    "* ステップ 1: Ground Truth は 10 枚の画像を 人間のアノテーターに「テスト用」として送信します。これらのアノテーションが成功すると、次のステップに進みます。\n",
    "* ステップ 2: `MaxConcurrentTaskCount - 10` 枚 (今回の場合、190 枚) の画像を人間のアノテーターに送信し、アクティブ・ラーニングのトレーニングバッチを取得します。\n",
    "* ステップ 3: 次の 200 枚の画像のバッチを人間のアノテーターに送信し、アクティブラーニングの検証用データセットを取得します。\n",
    "* ステップ 4a: 自動ラベリングを実行するためのニューラルネットの学習を行います。自動で可能な限り多くのラベル付を行います。\n",
    "* ステップ 4b: ラベル付けされなかったデータが有る場合、最大で 200 枚の画像を人間のアノテーターに送信します。\n",
    "* すべてのデータがラベル付けされるまで、ステップ 4a と 4b を繰り返します。\n",
    "\n",
    "`RUN_FULL_AL_DEMO==False`の場合、ステップ 1 と 2 のみ実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import shutil\n",
    "HUMAN_PRICE = 0.116\n",
    "AUTO_PRICE = 0.08\n",
    "\n",
    "try:\n",
    "    os.makedirs('ic_output_data/', exist_ok=False)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree('ic_output_data/')\n",
    "    \n",
    "S3_OUTPUT = boto3.client('sagemaker').describe_labeling_job(LabelingJobName=job_name)[\n",
    "    'OutputConfig']['S3OutputPath'] + job_name\n",
    "\n",
    "# 人間によるアノテーションの結果をダウンロードする\n",
    "!aws s3 cp {S3_OUTPUT + '/annotations/worker-response'} ic_output_data/worker-response --recursive --quiet\n",
    "worker_times = []\n",
    "worker_ids = []\n",
    "\n",
    "# ここまでのすべてのアノテーションイベントの時間とワーカー ID を収集する\n",
    "for annot_fname in glob.glob('ic_output_data/worker-response/**', recursive=True):\n",
    "    if annot_fname.endswith('json'):\n",
    "        with open(annot_fname, 'r') as f:\n",
    "            annot_data = json.load(f)\n",
    "        for answer in annot_data['answers']:\n",
    "            annot_time = datetime.strptime(\n",
    "                answer['submissionTime'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            annot_id = answer['workerId']\n",
    "            worker_times.append(annot_time)\n",
    "            worker_ids.append(annot_id)\n",
    "\n",
    "sort_ids = np.argsort(worker_times)\n",
    "worker_times = np.array(worker_times)[sort_ids]\n",
    "worker_ids = np.array(worker_ids)[sort_ids]\n",
    "cumulative_n_annots = np.cumsum([1 for _ in worker_times])\n",
    "\n",
    "# 各一意のワーカー ID について、アノテーションの数を数える\n",
    "annots_per_worker = np.zeros(worker_ids.size)\n",
    "ids_store = set()\n",
    "for worker_id_id, worker_id in enumerate(worker_ids):\n",
    "    ids_store.add(worker_id)\n",
    "    annots_per_worker[worker_id_id] = float(\n",
    "        cumulative_n_annots[worker_id_id]) / len(ids_store)\n",
    "    \n",
    "# 各クラス、反復ごとに、人間によるアノテーションの数を数える\n",
    "!aws s3 cp {S3_OUTPUT + '/annotations/consolidated-annotation/consolidation-response'} ic_output_data/consolidation-response --recursive --quiet\n",
    "consolidated_classes = defaultdict(list)\n",
    "consolidation_times = {}\n",
    "consolidated_cost_times = []\n",
    "\n",
    "for consolidated_fname in glob.glob('ic_output_data/consolidation-response/**', recursive=True):\n",
    "    if consolidated_fname.endswith('json'):\n",
    "        iter_id = int(consolidated_fname.split('/')[-2][-1])\n",
    "        # 反復時間として、直近の統合イベントの時間を格納する\n",
    "        iter_time = datetime.strptime(consolidated_fname.split('/')[-1], '%Y-%m-%d_%H:%M:%S.json')\n",
    "        if iter_id in consolidation_times:\n",
    "            consolidation_times[iter_id] = max(consolidation_times[iter_id], iter_time)\n",
    "        else:\n",
    "            consolidation_times[iter_id] = iter_time\n",
    "        consolidated_cost_times.append(iter_time)\n",
    "                                      \n",
    "        with open(consolidated_fname, 'r') as f:\n",
    "            consolidated_data = json.load(f)\n",
    "        for consolidation in consolidated_data:\n",
    "            consolidation_class = consolidation['consolidatedAnnotation']['content'][\n",
    "                'category-metadata']['class-name']\n",
    "            consolidated_classes[iter_id].append(consolidation_class)\n",
    "total_human_labels = sum([len(annots) for annots in consolidated_classes.values()])\n",
    "            \n",
    "# 各クラス、反復ごとに、機会によるアノテーションの数を数える\n",
    "!aws s3 cp {S3_OUTPUT + '/activelearning'} ic_output_data/activelearning --recursive --quiet\n",
    "auto_classes = defaultdict(list)\n",
    "auto_times = {}\n",
    "auto_cost_times = []\n",
    "\n",
    "for auto_fname in glob.glob('ic_output_data/activelearning/**', recursive=True):\n",
    "    if auto_fname.endswith('auto_annotator_output.txt'):\n",
    "        iter_id = int(auto_fname.split('/')[-3])\n",
    "        with open(auto_fname, 'r') as f:\n",
    "            annots = [' '.join(l.split()[1:]) for l in f.readlines()]\n",
    "        for annot in annots:\n",
    "            annot = json.loads(annot)\n",
    "            time_str = annot['category-metadata']['creation-date']\n",
    "            auto_time = datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "            auto_class = annot['category-metadata']['class-name']\n",
    "            auto_classes[iter_id].append(auto_class)\n",
    "            if iter_id in auto_times:\n",
    "                auto_times[iter_id] = max(auto_times[iter_id], auto_time)\n",
    "            else:\n",
    "                auto_times[iter_id] = auto_time\n",
    "            auto_cost_times.append(auto_time)\n",
    "                \n",
    "total_auto_labels = sum([len(annots) for annots in auto_classes.values()])\n",
    "n_iters = max(len(auto_times), len(consolidation_times))\n",
    "\n",
    "def get_training_job_data(training_job_name):\n",
    "    logclient = boto3.client('logs')\n",
    "    log_group_name = '/aws/sagemaker/TrainingJobs'\n",
    "    log_stream_name = logclient.describe_log_streams(logGroupName=log_group_name,\n",
    "        logStreamNamePrefix=training_job_name)['logStreams'][0]['logStreamName']\n",
    "    train_log = logclient.get_log_events(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamName=log_stream_name,\n",
    "        startFromHead=True\n",
    "    )\n",
    "    events = train_log['events']\n",
    "    next_token = train_log['nextForwardToken']\n",
    "    while True:\n",
    "        train_log = logclient.get_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=log_stream_name,\n",
    "            startFromHead=True,\n",
    "            nextToken=next_token\n",
    "        )\n",
    "        if train_log['nextForwardToken'] == next_token:\n",
    "            break\n",
    "        events = events + train_log['events']\n",
    "\n",
    "    errors = []\n",
    "    for event in events:\n",
    "        msg = event['message']\n",
    "        if 'Final configuration' in msg:\n",
    "            num_samples = int(msg.split('num_training_samples\\': u\\'')[1].split('\\'')[0])\n",
    "        elif 'Validation-accuracy' in msg:\n",
    "            errors.append(float(msg.split('Validation-accuracy=')[1]))\n",
    "\n",
    "    errors = 1 - np.array(errors)\n",
    "    return num_samples, errors\n",
    "\n",
    "training_data = !aws s3 ls {S3_OUTPUT + '/training/'} --recursive\n",
    "training_sizes = []\n",
    "training_errors = []\n",
    "training_iters = []\n",
    "for line in training_data:\n",
    "    if line.split('/')[-1] == 'model.tar.gz':\n",
    "        training_job_name = line.split('/')[-3]\n",
    "        n_samples, errors = get_training_job_data(training_job_name)\n",
    "        training_sizes.append(n_samples)\n",
    "        training_errors.append(errors)\n",
    "        training_iters.append(int(line.split('/')[-5]))\n",
    "\n",
    "plt.figure(facecolor='white', figsize=(14, 4), dpi=100)\n",
    "ax = plt.subplot(131)\n",
    "plt.title('Label counts ({} human, {} auto)'.format(\n",
    "    total_human_labels, total_auto_labels))\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "for iter_id in consolidated_classes.keys():\n",
    "    bottom = 0\n",
    "    class_counter = Counter(consolidated_classes[iter_id])\n",
    "    for cname_id, cname in enumerate(CLASS_LIST):\n",
    "        if iter_id == 1:\n",
    "            plt.bar(iter_id, class_counter[cname], width=.4, bottom=bottom,\n",
    "                label=cname, color=cmap(cname_id / float(len(CLASS_LIST)-1)))\n",
    "        else:\n",
    "            plt.bar(iter_id, class_counter[cname], width=.4, bottom=bottom,\n",
    "                color=cmap(cname_id / float(len(CLASS_LIST)-1)))\n",
    "\n",
    "        bottom += class_counter[cname]\n",
    "        \n",
    "for iter_id in auto_classes.keys():\n",
    "    bottom = 0\n",
    "    class_counter = Counter(auto_classes[iter_id])\n",
    "    for cname_id, cname in enumerate(CLASS_LIST):\n",
    "        plt.bar(iter_id + .4, class_counter[cname], width=.4, bottom=bottom, color=cmap(cname_id / float(len(CLASS_LIST)-1)))\n",
    "        bottom += class_counter[cname]\n",
    "\n",
    "tick_labels_human = ['Iter {}, human'.format(iter_id + 1) for iter_id in range(n_iters)]\n",
    "tick_labels_auto = ['Iter {}, auto'.format(iter_id + 1) for iter_id in range(n_iters)]\n",
    "tick_locations_human = np.arange(n_iters) + 1\n",
    "tick_locations_auto = tick_locations_human + .4\n",
    "tick_labels = np.concatenate([[tick_labels_human[idx], tick_labels_auto[idx]] for idx in range(n_iters)])\n",
    "tick_locations = np.concatenate([[tick_locations_human[idx], tick_locations_auto[idx]] for idx in range(n_iters)])\n",
    "plt.xticks(tick_locations, tick_labels, rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel('Count')\n",
    "\n",
    "ax = plt.subplot(132)\n",
    "total_human = 0\n",
    "total_auto = 0\n",
    "for iter_id in range(1, n_iters + 1):\n",
    "    cost_human = len(consolidated_classes[iter_id]) * HUMAN_PRICE\n",
    "    cost_auto = len(auto_classes[iter_id]) * AUTO_PRICE\n",
    "    total_human += cost_human\n",
    "    total_auto += cost_auto\n",
    "    \n",
    "    plt.bar(iter_id, cost_human, width=.8, color='gray',\n",
    "            hatch='/', edgecolor='k', label='human' if iter_id==1 else None)\n",
    "    plt.bar(iter_id, cost_auto, bottom=cost_human,\n",
    "            width=.8, color='gray', edgecolor='k', label='auto' if iter_id==1 else None)\n",
    "plt.title('Annotation costs (\\${:.2f} human, \\${:.2f} auto)'.format(\n",
    "    total_human, total_auto))\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Cost in dollars')\n",
    "plt.legend()\n",
    "\n",
    "if len(training_sizes) > 0:\n",
    "    plt.subplot(133)\n",
    "    plt.title('Active learning training curves')\n",
    "    plt.grid(True)\n",
    "\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    n_all = len(training_sizes)\n",
    "    for iter_id_id, (iter_id, size, errs) in enumerate(zip(training_iters, training_sizes, training_errors)):\n",
    "        plt.plot(errs, label='Iter {}, auto'.format(iter_id + 1), color=cmap(iter_id_id / max(1, (n_all-1))))\n",
    "        plt.legend()\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Training epoch')\n",
    "    plt.ylabel('Validation error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth ラベリングジョブの結果を分析する\n",
    "**このセクションは完了に 20 分程度かかります。**\n",
    "\n",
    "ジョブの実行が終了すると（**`sagemaker_client.describe_labeling_job` でジョブの完了が表示されていることを確認してください！**）、結果を分析するときとなります。 [ジョブの進捗状況を監視する](#ジョブの進捗状況を監視する) セクションのグラフは分析の一部を成します。このセクションでは、`output.manifest` にすべて含まれた、結果に対するさらなるインサイトを得ます。output.manifest の場所は、`AWS Console > SageMaker > Ground Truth > ラベリングジョブ > [あなたのジョブ]` の「出力データセットの場所」配下の `manifests/output` から見つけることができます。 下記のセルでは、コードによって取得します。\n",
    "\n",
    "## 出力マニフェストファイルの結果を後処理する\n",
    "ジョブが完了したので、output.manifest をダウンロードして後処理を行い、次の配列を作ります：\n",
    "* `img_uris` は Ground Truth でアノテーションされたすべての画像の S3 URI の配列です。\n",
    "* `labels` は `img_uris` の画像の Ground Truth のラベルの配列です。\n",
    "* `confidences` は `labels` の各ラベルの確信度の配列です。\n",
    "* `human` はフラグの配列であり、各インデックスの画像において、1 は人間によるアノテーションであること、0 は Ground Truth の自動ラベリングでアノテーションされたことを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力マニフェストのアノテーションの結果をダウンロードします\n",
    "OUTPUT_MANIFEST = \"s3://{}/{}/output/{}/manifests/output/output.manifest\".format(\n",
    "    BUCKET, EXP_NAME, job_name\n",
    ")\n",
    "\n",
    "!aws s3 cp {OUTPUT_MANIFEST} 'output.manifest'\n",
    "\n",
    "with open(\"output.manifest\", \"r\") as f:\n",
    "    output = [json.loads(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# 上述の配列を作ります\n",
    "img_uris = [None] * len(output)\n",
    "confidences = np.zeros(len(output))\n",
    "groundtruth_labels = [None] * len(output)\n",
    "human = np.zeros(len(output))\n",
    "\n",
    "# マニフェストに対応するジョブ名を取得します\n",
    "keys = list(output[0].keys())\n",
    "metakey = keys[np.where([(\"-metadata\" in k) for k in keys])[0][0]]\n",
    "jobname = metakey[:-9]\n",
    "\n",
    "# データを抽出します\n",
    "for datum_id, datum in enumerate(output):\n",
    "    img_uris[datum_id] = datum[\"source-ref\"]\n",
    "    groundtruth_labels[datum_id] = str(datum[metakey][\"class-name\"])\n",
    "    confidences[datum_id] = datum[metakey][\"confidence\"]\n",
    "    human[datum_id] = int(datum[metakey][\"human-annotated\"] == \"yes\")\n",
    "groundtruth_labels = np.array(groundtruth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスに対するヒストグラムを作成する\n",
    "それでは、クラスに対するヒストグラムを作成してみましょう。次のセルは、3 つの図を作成します：\n",
    "* 左の図は、各視覚的なカテゴリに属するアノテーションされた画像の数をプロットします。これらのカテゴリは降順にソートされています。各バーは 'human' と 'machine' の部分に分けられ、それぞれ何枚の画像が人手もしくは自動ラベリングのメカニズムでアノテーションされたかを示しています。\n",
    "\n",
    "* 真ん中の図は、左の図と同じですが、y 軸が対数スケールになっています。これにより、他と桁数が異なる画像を持ったカテゴリがあるような、偏ったデータセットを可視化することができます。\n",
    "\n",
    "* 右の図は、各カテゴリの画像の平均の確信度を、人間と自動アノテーションそれぞれで表しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各クラスのアノテーションの数を計算する\n",
    "n_classes = len(set(groundtruth_labels))\n",
    "sorted_clnames, class_sizes = zip(*Counter(groundtruth_labels).most_common(n_classes))\n",
    "\n",
    "# 人によるアノテーションの数を数える\n",
    "human_sizes = [human[groundtruth_labels == clname].sum() for clname in sorted_clnames]\n",
    "class_sizes = np.array(class_sizes)\n",
    "human_sizes = np.array(human_sizes)\n",
    "\n",
    "# 各クラスの平均のアノテーションの確信度を計算する\n",
    "human_confidences = np.array(\n",
    "    [confidences[np.logical_and(groundtruth_labels == clname, human)] for clname in sorted_clnames]\n",
    ")\n",
    "machine_confidences = [\n",
    "    confidences[np.logical_and(groundtruth_labels == clname, 1 - human)]\n",
    "    for clname in sorted_clnames\n",
    "]\n",
    "\n",
    "# アノテーションされた画像が無いクラスには、0 の平均確信度をセットする\n",
    "for class_id in range(n_classes):\n",
    "    if human_confidences[class_id].size == 0:\n",
    "        human_confidences[class_id] = np.array([0])\n",
    "    if machine_confidences[class_id].size == 0:\n",
    "        machine_confidences[class_id] = np.array([0])\n",
    "\n",
    "plt.figure(figsize=(9, 3), facecolor=\"white\", dpi=100)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Annotation histogram\")\n",
    "plt.bar(range(n_classes), human_sizes, color=\"gray\", hatch=\"/\", edgecolor=\"k\", label=\"human\")\n",
    "plt.bar(\n",
    "    range(n_classes),\n",
    "    class_sizes - human_sizes,\n",
    "    bottom=human_sizes,\n",
    "    color=\"gray\",\n",
    "    edgecolor=\"k\",\n",
    "    label=\"machine\",\n",
    ")\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90)\n",
    "plt.ylabel(\"Annotation Count\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Annotation histogram (logscale)\")\n",
    "plt.bar(range(n_classes), human_sizes, color=\"gray\", hatch=\"/\", edgecolor=\"k\", label=\"human\")\n",
    "plt.bar(\n",
    "    range(n_classes),\n",
    "    class_sizes - human_sizes,\n",
    "    bottom=human_sizes,\n",
    "    color=\"gray\",\n",
    "    edgecolor=\"k\",\n",
    "    label=\"machine\",\n",
    ")\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Mean confidences\")\n",
    "plt.bar(\n",
    "    np.arange(n_classes),\n",
    "    [conf.mean() for conf in human_confidences],\n",
    "    color=\"gray\",\n",
    "    hatch=\"/\",\n",
    "    edgecolor=\"k\",\n",
    "    width=0.4,\n",
    ")\n",
    "plt.bar(\n",
    "    np.arange(n_classes) + 0.4,\n",
    "    [conf.mean() for conf in machine_confidences],\n",
    "    color=\"gray\",\n",
    "    edgecolor=\"k\",\n",
    "    width=0.4,\n",
    ")\n",
    "plt.xticks(range(n_classes), sorted_clnames, rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アノテーションされた画像を表示する\n",
    "どんなデータサイエンスのタスクにおいても、結果をプロットして検査し、意味が通っているかを確認することは非常に重要です。このために、ここでは、\n",
    "1. Ground Truth がアノテーションした入力画像をダウンロードし、\n",
    "2. カテゴリとアノテーションを人か自動ラベリングで行ったかによって分け、\n",
    "3. それぞれの画像をプロットします。\n",
    "\n",
    "入力画像は次のセルで指定した `LOCAL_IMAGE_DIR` にダウンロードします。もしこのディレクトリにすでに同じ名前の入力画像があれば、改めて画像をダウンロードすることはしませんので、注意してください。\n",
    "\n",
    "もしデータセットが大きく、**すべての**画像をダウンロードして表示したくない場合は、単に `DATASET_SIZE` を小さい値にしてください。プロットするデータをランダムに抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_IMG_DIR = '<< choose a local directory name to download the images to >>' # 画像をダウンロードするローカルのディレクトリに置き換えてください\n",
    "assert LOCAL_IMG_DIR != '<< choose a local directory name to download the images to >>', 'Please provide a local directory name'\n",
    "DATASET_SIZE = len(img_uris) # データセットが1万枚以上の画像になる場合、適切な数に変更してください。\n",
    "\n",
    "subset_ids = np.random.choice(range(len(img_uris)), DATASET_SIZE, replace=False)\n",
    "img_uris = [img_uris[idx] for idx in subset_ids]\n",
    "groundtruth_labels = groundtruth_labels[subset_ids]\n",
    "confidences = confidences[subset_ids]\n",
    "human = human[subset_ids]\n",
    "\n",
    "img_fnames = [None] * len(output)\n",
    "for img_uri_id, img_uri in enumerate(img_uris):\n",
    "    target_fname = os.path.join(\n",
    "        LOCAL_IMG_DIR, img_uri.split('/')[-1])\n",
    "    if not os.path.isfile(target_fname):\n",
    "        !aws s3 cp {img_uri} {target_fname}\n",
    "    img_fnames[img_uri_id] = target_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数枚の出力サンプルを表示する\n",
    "次のセルは 2 つの図を生成します。1 つ目では、各カテゴリの `N_SHOW` 枚の画像を、人によるアノテーションとして表示します。2 つ目は、`N_SHOW` 枚の画像を、自動ラベリングによるアノテーションとして表示します。\n",
    "\n",
    "`N_SHOW` 枚以下の画像しかないカテゴリについては、その行はひょうじされません。デフォルトでは、を、`N_SHOW = 10`ですが、自由に異なる数に変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHOW = 10\n",
    "\n",
    "plt.figure(figsize=(3 * N_SHOW, 2 + 3 * n_classes), facecolor=\"white\", dpi=60)\n",
    "for class_name_id, class_name in enumerate(sorted_clnames):\n",
    "    class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, human))[0]\n",
    "    try:\n",
    "        show_ids = class_ids[:N_SHOW]\n",
    "    except ValueError:\n",
    "        print(\"Not enough human annotations to show for class: {}\".format(class_name))\n",
    "        continue\n",
    "    for show_id_id, show_id in enumerate(show_ids):\n",
    "        plt.subplot2grid((n_classes, N_SHOW), (class_name_id, show_id_id))\n",
    "        plt.title(\"Human Label: \" + class_name)\n",
    "        plt.imshow(imageio.imread(img_fnames[show_id]))  # image_fnames\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(3 * N_SHOW, 2 + 3 * n_classes), facecolor=\"white\", dpi=100)\n",
    "for class_name_id, class_name in enumerate(sorted_clnames):\n",
    "    class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, 1 - human))[0]\n",
    "    try:\n",
    "        show_ids = np.random.choice(class_ids, N_SHOW, replace=False)\n",
    "    except ValueError:\n",
    "        print(\"Not enough machine annotations to show for class: {}\".format(class_name))\n",
    "        continue\n",
    "    for show_id_id, show_id in enumerate(show_ids):\n",
    "        plt.subplot2grid((n_classes, N_SHOW), (class_name_id, show_id_id))\n",
    "        plt.title(\"Auto Label: \" + class_name)\n",
    "        plt.imshow(imageio.imread(img_fnames[show_id]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### すべての結果を表示する\n",
    "最後に、すべての結果を大きな PDF ファイルに表示します。（`ground_truth.pdf`の名前の）PDF は各ページに 100 枚の画像を表示します。各ページの画像はすべて同じカテゴリに属し、人手か自動ラベリングによるアノテーションのいずれかになります。この PDF を使うことで、正確にどの画像がどのクラスにアノテーションされたかをひと目で見る事ができます。\n",
    "\n",
    "この処理は少し時間がかかり、生成された PDF はとても大きくなるかもしれません。1000 枚ほどの画像であれば、数分で 10MB ほどの PDF になります。各カテゴリのサンプルの数を制限したい場合、`N_SHOW_PER_CLASS` を小さい数にセットしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHOW_PER_CLASS = np.inf\n",
    "plt.figure(figsize=(10, 10), facecolor=\"white\", dpi=100)\n",
    "\n",
    "with PdfPages(\"ground_truth.pdf\") as pdf:\n",
    "    for class_name in sorted_clnames:\n",
    "        # 人によって class_name とアノテーションされた画像をプロットする\n",
    "        plt.clf()\n",
    "        plt.text(0.1, 0.5, s=\"Images annotated as {} by humans\".format(class_name), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, human))[0]\n",
    "        for img_id_id, img_id in enumerate(class_ids):\n",
    "            if img_id_id == N_SHOW_PER_CLASS:\n",
    "                break\n",
    "            if img_id_id % 100 == 0:\n",
    "                pdf.savefig()\n",
    "                plt.clf()\n",
    "                print(\n",
    "                    \"Plotting human annotations of {}, {}/{}...\".format(\n",
    "                        class_name, (img_id_id + 1), min(len(class_ids), N_SHOW_PER_CLASS)\n",
    "                    )\n",
    "                )\n",
    "            plt.subplot(10, 10, (img_id_id % 100) + 1)\n",
    "            plt.imshow(imageio.imread(img_fnames[img_id]), aspect=\"auto\")\n",
    "            plt.axis(\"off\")\n",
    "        pdf.savefig()\n",
    "\n",
    "        # 機会によって class_name とアノテーションされた画像をプロットする\n",
    "        plt.clf()\n",
    "        plt.text(0.1, 0.5, s=\"Images annotated as {} by machines\".format(class_name), fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        class_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, 1 - human))[\n",
    "            0\n",
    "        ]\n",
    "        for img_id_id, img_id in enumerate(class_ids):\n",
    "            if img_id_id == N_SHOW_PER_CLASS:\n",
    "                break\n",
    "            if img_id_id % 100 == 0:\n",
    "                pdf.savefig()\n",
    "                plt.clf()\n",
    "                print(\n",
    "                    \"Plotting machine annotations of {}, {}/{}...\".format(\n",
    "                        class_name, (img_id_id + 1), min(len(class_ids), N_SHOW_PER_CLASS)\n",
    "                    )\n",
    "                )\n",
    "            plt.subplot(10, 10, (img_id_id % 100) + 1)\n",
    "            plt.imshow(imageio.imread(img_fnames[img_id]), aspect=\"auto\")\n",
    "            plt.axis(\"off\")\n",
    "        pdf.savefig()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth の結果を、既知の事前にラベル付けされたデータと比較する\n",
    "このセクションは完了に 5 分程度かかります。\n",
    "\n",
    "時々（例えば、システムのベンチマークを行う場合など）、代替となるラベル付けされたデータセットがある場合があります。例えば、Open Images のデータにはプロのアノテーションワークフォースによって注意深くアノテーションされています。これらを使うことで、 Ground Truth のラベル付けした結果を事前のラベル付けされた結果と比較するさらなる分析を行うことが出来ます。この際、人手によってラベル付けされた画像は殆どの場合 100% の正確性が無いことを心に留めておいてください。そのため、ラベリングの正確性を「Ground Truth のラベルが（絶対的な意味で）どれほどよいか」と考えるのではなく、「ある標準またはラベルのセットにしっかりと沿っているか」と考える方が良いです。\n",
    "\n",
    "## 精度を計算する\n",
    "このセルでは、標準のラベルに対して、Ground Truth の精度を計算します。\n",
    "\n",
    "\n",
    "[データの準備](#データの準備)では、 どの画像がどのカテゴリに属するかを示す `ims` ディレクトリを作成しました。それを `standard_labels[i]` が `i` 番目の画像のラベルを表すようなに `standard_labels` に変換し、対応する `groundtruth_labels[i]` が特定できるようにします。\n",
    "\n",
    "これにより、混同行列をプロットして、Ground Truth によるラベルが標準のものにどれほどよく沿っているかを評価することができます。ここでは、全体のデータセットに対して混合行列をプロットし、人手によるアノテーションと自動アノテーションの行列を分けるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    cm, classes, title=\"Confusion matrix\", normalize=False, cmap=plt.cm.Blues\n",
    "):\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \"d\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j].astype(int), fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# 'ims' ディレクトリ（クラス名から画像のマッピング）を画像クラスのリストに変換する\n",
    "standard_labels = []\n",
    "for img_uri in img_uris:\n",
    "    img_uri = img_uri.split(\"/\")[-1].split(\".\")[0]\n",
    "    standard_label = [cname for cname, imgs_in_cname in ims.items() if img_uri in imgs_in_cname][0]\n",
    "    standard_labels.append(standard_label)\n",
    "standard_labels = np.array(standard_labels)\n",
    "\n",
    "# データセット全体の混合行列をプロットする\n",
    "plt.figure(facecolor=\"white\", figsize=(12, 4), dpi=100)\n",
    "plt.subplot(131)\n",
    "mean_err = 100 - np.mean(standard_labels == groundtruth_labels) * 100\n",
    "cnf_matrix = confusion_matrix(standard_labels, groundtruth_labels)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix,\n",
    "    classes=sorted(ims.keys()),\n",
    "    title=\"Full annotation set error {:.2f}%\".format(mean_err),\n",
    "    normalize=False,\n",
    ")\n",
    "\n",
    "# 人によりアノテーションした Ground Truth のラベルの混合行列をプロットする\n",
    "plt.subplot(132)\n",
    "mean_err = 100 - np.mean(standard_labels[human == 1.0] == groundtruth_labels[human == 1.0]) * 100\n",
    "cnf_matrix = confusion_matrix(standard_labels[human == 1.0], groundtruth_labels[human == 1.0])\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix,\n",
    "    classes=sorted(ims.keys()),\n",
    "    title=\"Human annotation set (size {}) error {:.2f}%\".format(int(sum(human)), mean_err),\n",
    "    normalize=False,\n",
    ")\n",
    "\n",
    "# 自動でアノテーションした Ground Truth のラベルの混合行列をプロットする\n",
    "if sum(human == 0.0) > 0:\n",
    "    plt.subplot(133)\n",
    "    mean_err = (\n",
    "        100 - np.mean(standard_labels[human == 0.0] == groundtruth_labels[human == 0.0]) * 100\n",
    "    )\n",
    "    cnf_matrix = confusion_matrix(standard_labels[human == 0.0], groundtruth_labels[human == 0.0])\n",
    "    np.set_printoptions(precision=2)\n",
    "    plot_confusion_matrix(\n",
    "        cnf_matrix,\n",
    "        classes=sorted(ims.keys()),\n",
    "        title=\"Auto-annotation set (size {}) error {:.2f}%\".format(\n",
    "            int(len(human) - sum(human)), mean_err\n",
    "        ),\n",
    "        normalize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正誤のアノテーションをプロットする\n",
    "\n",
    "このセルは結果全体のプロットを繰り返します。ただし、推論結果を正しいほうから間違っている方にソートするので、すべての間違っている推論に対する標準のラベルを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHOW_PER_CLASS = np.inf\n",
    "plt.figure(figsize=(10, 10), facecolor=\"white\", dpi=100)\n",
    "\n",
    "with PdfPages(\"ground_truth_benchmark.pdf\") as pdf:\n",
    "    for class_name in sorted_clnames:\n",
    "        human_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, human))[0]\n",
    "        auto_ids = np.where(np.logical_and(np.array(groundtruth_labels) == class_name, 1 - human))[\n",
    "            0\n",
    "        ]\n",
    "        for class_ids_id, class_ids in enumerate([human_ids, auto_ids]):\n",
    "            plt.clf()\n",
    "            plt.text(\n",
    "                0.1,\n",
    "                0.5,\n",
    "                s=\"Images annotated as {} by {}\".format(\n",
    "                    class_name, \"humans\" if class_ids_id == 0 else \"machines\"\n",
    "                ),\n",
    "                fontsize=20,\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            good_ids = class_ids[\n",
    "                np.where(standard_labels[class_ids] == groundtruth_labels[class_ids])[0]\n",
    "            ]\n",
    "            bad_ids = class_ids[\n",
    "                np.where(standard_labels[class_ids] != groundtruth_labels[class_ids])[0]\n",
    "            ]\n",
    "            for img_id_id, img_id in enumerate(np.concatenate([good_ids, bad_ids])):\n",
    "                if img_id_id == N_SHOW_PER_CLASS:\n",
    "                    break\n",
    "                if img_id_id % 100 == 0:\n",
    "                    pdf.savefig()\n",
    "                    plt.clf()\n",
    "                    print(\n",
    "                        \"Plotting annotations of {}, {}/{}...\".format(\n",
    "                            class_name, img_id_id, min(len(class_ids), N_SHOW_PER_CLASS)\n",
    "                        )\n",
    "                    )\n",
    "                ax = plt.subplot(10, 10, (img_id_id % 100) + 1)\n",
    "                plt.imshow(imageio.imread(img_fnames[img_id]), aspect=\"auto\")\n",
    "                plt.axis(\"off\")\n",
    "                if img_id_id < len(good_ids):\n",
    "                    # Draw a green border around the image.\n",
    "                    rec = matplotlib.patches.Rectangle(\n",
    "                        (0, 0), 1, 1, lw=10, edgecolor=\"green\", fill=False, transform=ax.transAxes\n",
    "                    )\n",
    "                else:\n",
    "                    # Draw a red border around the image.\n",
    "                    rec = matplotlib.patches.Rectangle(\n",
    "                        (0, 0), 1, 1, lw=10, edgecolor=\"red\", fill=False, transform=ax.transAxes\n",
    "                    )\n",
    "                ax.add_patch(rec)\n",
    "            pdf.savefig()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth を使った画像分類器のトレーニング\n",
    "ここまでで私達は完全なラベル付けされたデータセットを作成し、ここから最初のほうで定義したカテゴリに基づく画像を分類する機械学習モデルのトレーニングを行うことができます。ラベリングジョブの**拡張マニフェスト** を使うことで、さらなる変換や処理を行う必要なく、このトレーニングを実行できます。拡張マニフェストに関する完全な説明は、別の[サンプルノートブック](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/ground_truth_labeling_jobs/object_detection_augmented_manifest_training/object_detection_augmented_manifest_training.ipynb)を参照してください。\n",
    "\n",
    "**注：** 高い精度を持つニューラルネットの学習には、ハイパーパラメータの選択を入念に行う必要があることが多いです。今回の場合、データセットに十分に良いハイパーパラメータを人手で指定しています。今回のニューラルネットは**100 データに対しておおよそ 60%、1000 データに対して 95% 以上の**精度を持つことができるはずです。 真新しいデータに対してニューラルネットワークの学習を行うには、[SageMaker モデルのハイパーパラメータチューニング](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)を利用することを検討してください。\n",
    "\n",
    "最初に、拡張マニフェストを学習用と検証用に 80/20 の割合で分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.manifest\", \"r\") as f:\n",
    "    output = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "# 出力結果の配列内をシャッフルします\n",
    "np.random.shuffle(output)\n",
    "\n",
    "dataset_size = len(output)\n",
    "train_test_split_index = round(dataset_size * 0.8)\n",
    "\n",
    "train_data = output[:train_test_split_index]\n",
    "validation_data = output[train_test_split_index:]\n",
    "\n",
    "num_training_samples = 0\n",
    "with open(\"train.manifest\", \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")\n",
    "        num_training_samples += 1\n",
    "\n",
    "with open(\"validation.manifest\", \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、これらのマニフェストファイルを冒頭で定義した S3 バケットにアップロードし、トレーニングジョブで使用できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\"train.manifest\", BUCKET, EXP_NAME + \"/train.manifest\")\n",
    "s3.upload_file(\"validation.manifest\", BUCKET, EXP_NAME + \"/validation.manifest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一意なジョブ名を作成します\n",
    "nn_job_name_prefix = \"groundtruth-augmented-manifest-demo\"\n",
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "nn_job_name = nn_job_name_prefix + timestamp\n",
    "\n",
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    \"image-classification\", boto3.Session().region_name\n",
    ")\n",
    "\n",
    "training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": training_image, \"TrainingInputMode\": \"Pipe\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": \"s3://{}/{}/output/\".format(BUCKET, EXP_NAME)},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.p3.2xlarge\", \"VolumeSizeInGB\": 50},\n",
    "    \"TrainingJobName\": nn_job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"epochs\": \"30\",\n",
    "        \"image_shape\": \"3,224,224\",\n",
    "        \"learning_rate\": \"0.01\",\n",
    "        \"lr_scheduler_step\": \"10,20\",\n",
    "        \"mini_batch_size\": \"16\",\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"num_layers\": \"18\",\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"resize\": \"224\",\n",
    "        \"use_pretrained_model\": \"1\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/{}\".format(BUCKET, EXP_NAME, \"train.manifest\"),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": [\"source-ref\", \"category\"],\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/{}\".format(BUCKET, EXP_NAME, \"validation.manifest\"),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": [\"source-ref\", \"category\"],\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、SageMaker のトレーニングジョブを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_client.create_training_job(**training_params)\n",
    "\n",
    "# トレーニングジョブが開始したことを確認します\n",
    "print(\"Transform job started\")\n",
    "while True:\n",
    "    status = sagemaker_client.describe_training_job(TrainingJobName=nn_job_name)[\n",
    "        \"TrainingJobStatus\"\n",
    "    ]\n",
    "    if status == \"Completed\":\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルをデプロイする\n",
    "\n",
    "ここまでで私達は完全なラベル付けされたデータセットを作成し、モデルの学習をしたので、このモデルを使って推論を実行しましょう。\n",
    "\n",
    "現在の画像分類は、.jpg と .png 形式の画像のみを推論のインプットとしてサポートします。出力はすべてのクラスに対する確率値を JSON 形式、もしくはバッチ形式の場合は JSON Lines 形式で表したものになります。\n",
    "\n",
    "このセクションは複数のステップから成っています：\n",
    "\n",
    "    モデルの生成 - トレーニングアウトプットに対してモデルを生成します\n",
    "    バッチ変換 - バッチ推論のための変換ジョブを生成します\n",
    "    リアルタイム推論のためのモデルのホスト - 推論エンドポイントを生成し、リアルタイム推論を実施します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "model_name = \"groundtruth-demo-ic-model\" + timestamp\n",
    "print(model_name)\n",
    "info = sagemaker_client.describe_training_job(TrainingJobName=nn_job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    \"Image\": training_image,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バッチ変換\n",
    "それでは、上で作成したモデルを使ってバッチ推論を行うための SageMaker バッチ変換ジョブを作成します。\n",
    "\n",
    "### テストデータをダウンロードする\n",
    "まず、学習および検証データで試用しなかったテスト画像をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "batch_job_name = \"image-classification-model\" + timestamp\n",
    "batch_input = 's3://{}/{}/test/'.format(BUCKET, EXP_NAME)\n",
    "batch_output = 's3://{}/{}/{}/output/'.format(BUCKET, EXP_NAME, batch_job_name)\n",
    "\n",
    "# 各クラスから、ニューラルネットにとって新しい 2 枚の画像を、ローカルのバケットにコピーする\n",
    "test_images = []\n",
    "for class_id in ['/m/04szw', '/m/02xwb', '/m/0cd4d', '/m/07dm6', '/m/0152hh']:\n",
    "    test_images.extend([label[0] + '.jpg' for label in all_labels if (label[2] == class_id and label[3] == '1')][-2:])\n",
    "    \n",
    "!aws s3 rm $batch_input --recursive\n",
    "for test_img in test_images:\n",
    "    !aws s3 cp s3://open-images-dataset/test/{test_img} {batch_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"MaxConcurrentTransforms\": 16,\n",
    "    \"MaxPayloadInMB\": 6,\n",
    "    \"BatchStrategy\": \"SingleRecord\",\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/{}/output/\".format(BUCKET, EXP_NAME, batch_job_name)\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input}},\n",
    "        \"ContentType\": \"application/x-image\",\n",
    "        \"SplitType\": \"None\",\n",
    "        \"CompressionType\": \"None\",\n",
    "    },\n",
    "    \"TransformResources\": {\"InstanceType\": \"ml.p2.xlarge\", \"InstanceCount\": 1},\n",
    "}\n",
    "\n",
    "print(\"Transform job name: {}\".format(batch_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_client.create_transform_job(**request)\n",
    "\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "while True:\n",
    "    response = sagemaker_client.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    if status == \"Completed\":\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジョブが完了したあと、推論結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LIST = [\"Musical Instrument\", \"Fruit\", \"Cheetah\", \"Tiger\", \"Snowman\"]\n",
    "def get_label(out_fname):\n",
    "    !aws s3 cp {out_fname} .\n",
    "    print(out_fname)\n",
    "    with open(out_fname.split('/')[-1]) as f:\n",
    "        data = json.load(f)\n",
    "        index = np.argmax(data['prediction'])\n",
    "        probability = data['prediction'][index]\n",
    "    print(\"Result: label - \" + CLASS_LIST[index] + \", probability - \" + str(probability))\n",
    "    input_fname = out_fname.split('/')[-1][:-4]\n",
    "    return CLASS_LIST[index], probability, input_fname\n",
    "\n",
    "# Show prediction results.\n",
    "!rm test_inputs/*\n",
    "plt.figure(facecolor='white', figsize=(7, 15), dpi=100)\n",
    "outputs = !aws s3 ls {batch_output}\n",
    "outputs = [get_label(batch_output + prefix.split()[-1]) for prefix in outputs]\n",
    "outputs.sort(key=lambda pred: pred[1], reverse=True)\n",
    "\n",
    "for fname_id, (pred_cname, pred_conf, pred_fname) in enumerate(outputs):\n",
    "    !aws s3 cp {batch_input}{pred_fname} test_inputs/{pred_fname}\n",
    "    plt.subplot(5, 2, fname_id+1) \n",
    "    img = imageio.imread('test_inputs/{}'.format(pred_fname))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('{}\\nconfidence={:.2f}'.format(pred_cname, pred_conf))\n",
    "    \n",
    "if RUN_FULL_AL_DEMO:\n",
    "    warning = ''\n",
    "else:\n",
    "    warning = ('\\nNOTE: In this small demo we only used 80 images to train the neural network.\\n'\n",
    "               'The predictions will be far from perfect! Set RUN_FULL_AL_DEMO=True to see properly trained results.')\n",
    "plt.suptitle('Predictions sorted by confidence.{}'.format(warning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リアルタイム推論\n",
    "\n",
    "それでは、モデルをホストしてエンドポイントを作成し、リアルタイム推論を行います。\n",
    "\n",
    "このセクションは複数のステップから成っています：\n",
    "\n",
    "    エンドポイント設定を作成する - エンドポイントを定義する設定を作成します\n",
    "    エンドポイントを作成する - 設定を試用して推論エンドポイントを作成します\n",
    "    推論を実行する - エンドポイントを利用して、入力したデータに対して推論を実行します\n",
    "    クリーンアップ - エンドポイントとモデルを削除します\n",
    "\n",
    "### エンドポイント設定を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_config_name = job_name + \"-epc\" + timestamp\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint configuration name: {}\".format(endpoint_config_name))\n",
    "print(\"Endpoint configuration arn:  {}\".format(endpoint_config_response[\"EndpointConfigArn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンドポイントを作成する\n",
    "\n",
    "最後に、名前と先程定義した設定を使って、モデルを提供するためのエンドポイントを作成します。最終的には、エンドポイントは検証され、商用のアプリケーションに組み込まれます。この実行には、10分ほどかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_name = job_name + \"-ep\" + timestamp\n",
    "print(\"Endpoint name: {}\".format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker_client.create_endpoint(**endpoint_params)\n",
    "print(\"EndpointArn = {}\".format(endpoint_response[\"EndpointArn\"]))\n",
    "\n",
    "# エンドポイントの状態を取得する\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response[\"EndpointStatus\"]\n",
    "print(\"EndpointStatus = {}\".format(status))\n",
    "\n",
    "# 状態が変わるまで待つ\n",
    "sagemaker_client.get_waiter(\"endpoint_in_service\").wait(EndpointName=endpoint_name)\n",
    "\n",
    "# エンドポイントの状態を出力する\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response[\"EndpointStatus\"]\n",
    "print(\"Endpoint creation ended with EndpointStatus = {}\".format(status))\n",
    "\n",
    "if status != \"InService\":\n",
    "    raise Exception(\"Endpoint creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論を実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_inputs/{}\".format(test_images[0]), \"rb\") as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    ")\n",
    "\n",
    "# `response` は JSON 形式で取得されるため、展開する\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "# 出力された結果は、全てのクラスの確率値\n",
    "# 最大の確率を持つクラスを見つけ、出力する\n",
    "print(\"Model prediction is: {}\".format(CLASS_LIST[np.argmax(result)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、エンドポイントを削除して、クリーンアップします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "このノートブックでは、様々な概念についれ触れていきました！ここまで達成してきたことを振り返ってみましょう。まず、私達はラベルの無いデータセット（技術的には、データセットの作者によって事前にラベル付けされていましたが、このデモのために元のラベルは一旦破棄しています）。次に、SageMake Ground Truth のラベリングジョブを作成し、データセット内のすべての画像に対してラベルを付与しました。その後、このファイルを学習用と検証用に分割し、SageMaker の画像分類モデルをトレーニングしました。最後に、モデルのエンドポイントをホストし、元のデータセットで残しておいた画像に対して、エンドポイントを使ったリアルタイム推論を行いました。\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
